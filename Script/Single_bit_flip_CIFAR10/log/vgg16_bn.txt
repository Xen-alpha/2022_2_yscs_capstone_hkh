============================ PYTORCHFI INIT SUMMARY ==============================

Layer types allowing injections:
----------------------------------------------------------------------------------
   - all

Model Info:
----------------------------------------------------------------------------------
   - Shape of input into the model: (3 32 32 )
   - Batch Size: 256
   - CUDA Enabled: True

Layer Info:
----------------------------------------------------------------------------------
Layer #       Layer type  Dimensions         Weight Shape         Output Shape
----------------------------------------------------------------------------------
    0         Identity           4       ['No weights']       [1, 3, 32, 32]
    1           Conv2d           4        [64, 3, 3, 3]      [1, 64, 32, 32]
    2      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    3             ReLU           4       ['No weights']      [1, 64, 32, 32]
    4           Conv2d           4       [64, 64, 3, 3]      [1, 64, 32, 32]
    5      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    6             ReLU           4       ['No weights']      [1, 64, 32, 32]
    7        MaxPool2d           4       ['No weights']      [1, 64, 16, 16]
    8           Conv2d           4      [128, 64, 3, 3]     [1, 128, 16, 16]
    9      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   10             ReLU           4       ['No weights']     [1, 128, 16, 16]
   11           Conv2d           4     [128, 128, 3, 3]     [1, 128, 16, 16]
   12      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   13             ReLU           4       ['No weights']     [1, 128, 16, 16]
   14        MaxPool2d           4       ['No weights']       [1, 128, 8, 8]
   15           Conv2d           4     [256, 128, 3, 3]       [1, 256, 8, 8]
   16      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   17             ReLU           4       ['No weights']       [1, 256, 8, 8]
   18           Conv2d           4     [256, 256, 3, 3]       [1, 256, 8, 8]
   19      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   20             ReLU           4       ['No weights']       [1, 256, 8, 8]
   21           Conv2d           4     [256, 256, 3, 3]       [1, 256, 8, 8]
   22      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   23             ReLU           4       ['No weights']       [1, 256, 8, 8]
   24        MaxPool2d           4       ['No weights']       [1, 256, 4, 4]
   25           Conv2d           4     [512, 256, 3, 3]       [1, 512, 4, 4]
   26      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   27             ReLU           4       ['No weights']       [1, 512, 4, 4]
   28           Conv2d           4     [512, 512, 3, 3]       [1, 512, 4, 4]
   29      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   30             ReLU           4       ['No weights']       [1, 512, 4, 4]
   31           Conv2d           4     [512, 512, 3, 3]       [1, 512, 4, 4]
   32      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   33             ReLU           4       ['No weights']       [1, 512, 4, 4]
   34        MaxPool2d           4       ['No weights']       [1, 512, 2, 2]
   35           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   36      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   37             ReLU           4       ['No weights']       [1, 512, 2, 2]
   38           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   39      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   40             ReLU           4       ['No weights']       [1, 512, 2, 2]
   41           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   42      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   43             ReLU           4       ['No weights']       [1, 512, 2, 2]
   44        MaxPool2d           4       ['No weights']       [1, 512, 1, 1]
   45  AdaptiveAvgPool2d           4       ['No weights']       [1, 512, 1, 1]
   46           Linear           2          [4096, 512]            [1, 4096]
   47             ReLU           2       ['No weights']            [1, 4096]
   48          Dropout           2       ['No weights']            [1, 4096]
   49           Linear           2         [4096, 4096]            [1, 4096]
   50             ReLU           2       ['No weights']            [1, 4096]
   51          Dropout           2       ['No weights']            [1, 4096]
   52           Linear           2           [10, 4096]              [1, 10]
==================================================================================


===== Result =====
Quantization bits: 32
Layer #0: 312 / 9384 = 3.3248%
Layer #1: 310 / 9384 = 3.3035%
Layer #2: 307 / 9386 = 3.2708%
Layer #3: 310 / 9384 = 3.3035%
Layer #4: 310 / 9386 = 3.3028%
Layer #5: 311 / 9385 = 3.3138%
Layer #6: 312 / 9387 = 3.3237%
Layer #7: 310 / 9385 = 3.3031%
Layer #8: 308 / 9385 = 3.2818%
Layer #9: 308 / 9384 = 3.2822%
Layer #10: 310 / 9386 = 3.3028%
Layer #11: 311 / 9384 = 3.3142%
Layer #12: 308 / 9385 = 3.2818%
Layer #13: 310 / 9385 = 3.3031%
Layer #14: 309 / 9387 = 3.2918%
Layer #15: 310 / 9385 = 3.3031%
Layer #16: 308 / 9385 = 3.2818%
Layer #17: 310 / 9385 = 3.3031%
Layer #18: 311 / 9384 = 3.3142%
Layer #19: 312 / 9384 = 3.3248%
Layer #20: 313 / 9384 = 3.3355%
Layer #21: 308 / 9385 = 3.2818%
Layer #22: 311 / 9385 = 3.3138%
Layer #23: 310 / 9385 = 3.3031%
Layer #24: 305 / 9384 = 3.2502%
Layer #25: 309 / 9385 = 3.2925%
Layer #26: 310 / 9384 = 3.3035%
Layer #27: 311 / 9387 = 3.3131%
Layer #28: 308 / 9384 = 3.2822%
Layer #29: 312 / 9386 = 3.3241%
Layer #30: 310 / 9385 = 3.3031%
Layer #31: 310 / 9384 = 3.3035%
Layer #32: 310 / 9384 = 3.3035%
Layer #33: 310 / 9387 = 3.3024%
Layer #34: 309 / 9385 = 3.2925%
Layer #35: 308 / 9386 = 3.2815%
Layer #36: 310 / 9384 = 3.3035%
Layer #37: 310 / 9385 = 3.3031%
Layer #38: 310 / 9384 = 3.3035%
Layer #39: 309 / 9384 = 3.2928%
Layer #40: 311 / 9386 = 3.3134%
Layer #41: 311 / 9385 = 3.3138%
Layer #42: 309 / 9387 = 3.2918%
Layer #43: 310 / 9386 = 3.3028%
Layer #44: 311 / 9384 = 3.3142%
Layer #45: 309 / 9385 = 3.2925%
Layer #46: 310 / 9387 = 3.3024%
Layer #47: 310 / 9386 = 3.3028%
Layer #48: 310 / 9386 = 3.3028%
Layer #49: 310 / 9386 = 3.3028%
Layer #50: 310 / 9385 = 3.3031%
Layer #51: 310 / 9384 = 3.3035%
Layer #52: 419 / 9385 = 4.4646%
