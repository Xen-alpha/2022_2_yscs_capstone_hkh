{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "Differences:\n",
        "\n",
        "설정정 및및 모델델 불러오기기: exp_num\n",
        "실험험 준비하기기: LogFile: l.write 어쩌고"
      ],
      "metadata": {
        "id": "yLeD4uZVarAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JjyLauM_aq3w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz0hzpcrI078"
      },
      "source": [
        "## 시작하기 전에\n",
        "\n",
        "CIFAR-10 pretrained weight 받아오기\n",
        "\n",
        "1. https://github.com/huyvnphan/PyTorch_CIFAR10 중간의 구글 드라이브 링크에서 zip 파일을 다운 (약 1기가)\n",
        "2. 압축 해제 후 state_dicts 폴더를 구글 드라이브에 저장\n",
        "\n",
        "\n",
        "CIFAR-10 pretrained model 받아오기\n",
        "1. 아래 코드 실행\n",
        "\n",
        "\n",
        "몇 가지 오류를 수정한 PytorchFI 라이브러리 받아오기\n",
        "1. 아래아래 코드 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQle-d6LIop_",
        "outputId": "7d8cb07e-933c-4588-d1e4-b444e73af950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PyTorch_CIFAR10'...\n",
            "remote: Enumerating objects: 648, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 648 (delta 48), reused 34 (delta 33), pack-reused 583\u001b[K\n",
            "Receiving objects: 100% (648/648), 6.57 MiB | 16.46 MiB/s, done.\n",
            "Resolving deltas: 100% (243/243), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/WaiNaat/PyTorch_CIFAR10.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ihCv0gDJDDX",
        "outputId": "0d9baecb-8785-4ebd-88f8-bccddcef96b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorchfi'...\n",
            "remote: Enumerating objects: 472, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/241)\u001b[K\rremote: Counting objects:   1% (3/241)\u001b[K\rremote: Counting objects:   2% (5/241)\u001b[K\rremote: Counting objects:   3% (8/241)\u001b[K\rremote: Counting objects:   4% (10/241)\u001b[K\rremote: Counting objects:   5% (13/241)\u001b[K\rremote: Counting objects:   6% (15/241)\u001b[K\rremote: Counting objects:   7% (17/241)\u001b[K\rremote: Counting objects:   8% (20/241)\u001b[K\rremote: Counting objects:   9% (22/241)\u001b[K\rremote: Counting objects:  10% (25/241)\u001b[K\rremote: Counting objects:  11% (27/241)\u001b[K\rremote: Counting objects:  12% (29/241)\u001b[K\rremote: Counting objects:  13% (32/241)\u001b[K\rremote: Counting objects:  14% (34/241)\u001b[K\rremote: Counting objects:  15% (37/241)\u001b[K\rremote: Counting objects:  16% (39/241)\u001b[K\rremote: Counting objects:  17% (41/241)\u001b[K\rremote: Counting objects:  18% (44/241)\u001b[K\rremote: Counting objects:  19% (46/241)\u001b[K\rremote: Counting objects:  20% (49/241)\u001b[K\rremote: Counting objects:  21% (51/241)\u001b[K\rremote: Counting objects:  22% (54/241)\u001b[K\rremote: Counting objects:  23% (56/241)\u001b[K\rremote: Counting objects:  24% (58/241)\u001b[K\rremote: Counting objects:  25% (61/241)\u001b[K\rremote: Counting objects:  26% (63/241)\u001b[K\rremote: Counting objects:  27% (66/241)\u001b[K\rremote: Counting objects:  28% (68/241)\u001b[K\rremote: Counting objects:  29% (70/241)\u001b[K\rremote: Counting objects:  30% (73/241)\u001b[K\rremote: Counting objects:  31% (75/241)\u001b[K\rremote: Counting objects:  32% (78/241)\u001b[K\rremote: Counting objects:  33% (80/241)\u001b[K\rremote: Counting objects:  34% (82/241)\u001b[K\rremote: Counting objects:  35% (85/241)\u001b[K\rremote: Counting objects:  36% (87/241)\u001b[K\rremote: Counting objects:  37% (90/241)\u001b[K\rremote: Counting objects:  38% (92/241)\u001b[K\rremote: Counting objects:  39% (94/241)\u001b[K\rremote: Counting objects:  40% (97/241)\u001b[K\rremote: Counting objects:  41% (99/241)\u001b[K\rremote: Counting objects:  42% (102/241)\u001b[K\rremote: Counting objects:  43% (104/241)\u001b[K\rremote: Counting objects:  44% (107/241)\u001b[K\rremote: Counting objects:  45% (109/241)\u001b[K\rremote: Counting objects:  46% (111/241)\u001b[K\rremote: Counting objects:  47% (114/241)\u001b[K\rremote: Counting objects:  48% (116/241)\u001b[K\rremote: Counting objects:  49% (119/241)\u001b[K\rremote: Counting objects:  50% (121/241)\u001b[K\rremote: Counting objects:  51% (123/241)\u001b[K\rremote: Counting objects:  52% (126/241)\u001b[K\rremote: Counting objects:  53% (128/241)\u001b[K\rremote: Counting objects:  54% (131/241)\u001b[K\rremote: Counting objects:  55% (133/241)\u001b[K\rremote: Counting objects:  56% (135/241)\u001b[K\rremote: Counting objects:  57% (138/241)\u001b[K\rremote: Counting objects:  58% (140/241)\u001b[K\rremote: Counting objects:  59% (143/241)\u001b[K\rremote: Counting objects:  60% (145/241)\u001b[K\rremote: Counting objects:  61% (148/241)\u001b[K\rremote: Counting objects:  62% (150/241)\u001b[K\rremote: Counting objects:  63% (152/241)\u001b[K\rremote: Counting objects:  64% (155/241)\u001b[K\rremote: Counting objects:  65% (157/241)\u001b[K\rremote: Counting objects:  66% (160/241)\u001b[K\rremote: Counting objects:  67% (162/241)\u001b[K\rremote: Counting objects:  68% (164/241)\u001b[K\rremote: Counting objects:  69% (167/241)\u001b[K\rremote: Counting objects:  70% (169/241)\u001b[K\rremote: Counting objects:  71% (172/241)\u001b[K\rremote: Counting objects:  72% (174/241)\u001b[K\rremote: Counting objects:  73% (176/241)\u001b[K\rremote: Counting objects:  74% (179/241)\u001b[K\rremote: Counting objects:  75% (181/241)\u001b[K\rremote: Counting objects:  76% (184/241)\u001b[K\rremote: Counting objects:  77% (186/241)\u001b[K\rremote: Counting objects:  78% (188/241)\u001b[K\rremote: Counting objects:  79% (191/241)\u001b[K\rremote: Counting objects:  80% (193/241)\u001b[K\rremote: Counting objects:  81% (196/241)\u001b[K\rremote: Counting objects:  82% (198/241)\u001b[K\rremote: Counting objects:  83% (201/241)\u001b[K\rremote: Counting objects:  84% (203/241)\u001b[K\rremote: Counting objects:  85% (205/241)\u001b[K\rremote: Counting objects:  86% (208/241)\u001b[K\rremote: Counting objects:  87% (210/241)\u001b[K\rremote: Counting objects:  88% (213/241)\u001b[K\rremote: Counting objects:  89% (215/241)\u001b[K\rremote: Counting objects:  90% (217/241)\u001b[K\rremote: Counting objects:  91% (220/241)\u001b[K\rremote: Counting objects:  92% (222/241)\u001b[K\rremote: Counting objects:  93% (225/241)\u001b[K\rremote: Counting objects:  94% (227/241)\u001b[K\rremote: Counting objects:  95% (229/241)\u001b[K\rremote: Counting objects:  96% (232/241)\u001b[K\rremote: Counting objects:  97% (234/241)\u001b[K\rremote: Counting objects:  98% (237/241)\u001b[K\rremote: Counting objects:  99% (239/241)\u001b[K\rremote: Counting objects: 100% (241/241)\u001b[K\rremote: Counting objects: 100% (241/241), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 472 (delta 180), reused 112 (delta 92), pack-reused 231\u001b[K\n",
            "Receiving objects: 100% (472/472), 17.98 MiB | 39.42 MiB/s, done.\n",
            "Resolving deltas: 100% (268/268), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/WaiNaat/pytorchfi.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPE26QlAJDd_",
        "outputId": "d59a0cd0-c15a-47c5-94fb-d33b74e255a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RSDhkFSUJJYh"
      },
      "outputs": [],
      "source": [
        "import pytorchfi\n",
        "from pytorchfi.core import FaultInjection\n",
        "from pytorchfi.neuron_error_models import single_bit_flip_func\n",
        "\n",
        "from PyTorch_CIFAR10.cifar10_models.vgg import vgg11_bn, vgg13_bn, vgg16_bn, vgg19_bn\n",
        "from PyTorch_CIFAR10.cifar10_models.resnet import resnet18, resnet34, resnet50\n",
        "from PyTorch_CIFAR10.cifar10_models.densenet import densenet121, densenet161, densenet169\n",
        "from PyTorch_CIFAR10.cifar10_models.mobilenetv2 import mobilenet_v2\n",
        "from PyTorch_CIFAR10.cifar10_models.googlenet import googlenet\n",
        "from PyTorch_CIFAR10.cifar10_models.inception import inception_v3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQp0-9qPJODt"
      },
      "source": [
        "## 설정 및 모델 불러오기\n",
        "\n",
        "---\n",
        "\n",
        "`model_name`, `model`: 위 셀의 `PyTorch_CIFAR10.cifar10_models` 에서 `import` 한 것들 중 하나      \n",
        "`layer_type`: `['all']` 또는 `torch.nn.Modules`를 상속하는 클래스명으로 구성된 iterable   \n",
        "`layer_nums`: `['all']` 또는 0 이상의 정수로 구성된 배열    \n",
        "`corrupt_input_images`: `True`로 설정 시 inference 전 입력 이미지 자체에도 single bit flip 적용\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "75fT2KVaJQxh"
      },
      "outputs": [],
      "source": [
        "# 실험 환경 설정\n",
        "model_name = \"resnet18\"\n",
        "model = resnet18()\n",
        "save_dir = 'resnet18.txt'\n",
        "exp_num = 0\n",
        "\n",
        "seed = 1234\n",
        "\n",
        "batch_size = 256\n",
        "img_size = 32\n",
        "channels = 3\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "corrupt_input_images = True\n",
        "quant_bits = 32\n",
        "layer_type = ['all']\n",
        "#layer_type = [torch.nn.ReLU]\n",
        "#layer_type = [torch.nn.Conv2d]\n",
        "#layer_type = [torch.nn.BatchNorm2d]\n",
        "#layer_type = [torch.nn.MaxPool2d]\n",
        "#layer_type = [torch.nn.AdaptiveAvgPool2d]\n",
        "layer_nums = ['all']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4772Qy2ZJdw7",
        "outputId": "0b57cbb2-d47b-4b44-a068-b34be9ba87b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fad3cd9e710>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Kb48rrLDVLnS"
      },
      "outputs": [],
      "source": [
        "class add_input_layer(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, model, *args):\n",
        "        super().__init__(*args)\n",
        "        self.input_layer = torch.nn.Identity()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = self.input_layer(x)\n",
        "        output = self.model(input)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip \"/content/drive/MyDrive/state_dicts.zip\" -d \"/content/drive/MyDrive/state_dicts\""
      ],
      "metadata": {
        "id": "-nXGJ5y-0Soa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "aB5rWhsWFC7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80mibcAZJeIk"
      },
      "outputs": [],
      "source": [
        "# 모델 설정\n",
        "path = f\"/content/drive/MyDrive/state_dicts/state_dicts/{model_name}.pt\"\n",
        "model.load_state_dict(torch.load(path))\n",
        "\n",
        "if corrupt_input_images:\n",
        "    model = add_input_layer(model)\n",
        "\n",
        "if use_gpu: model.to(device='cuda')\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-p5YbxQJZw6"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjscQaFfJ2Uw",
        "outputId": "64869668-d5d2-40a5-eb68-9d1498fda640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Normalization statics from https://github.com/huyvnphan/PyTorch_CIFAR10/blob/master/data.py\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.4914, 0.4822, 0.4465], (0.2471, 0.2435, 0.2616))\n",
        "    ]\n",
        ")\n",
        "\n",
        "data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "dataset = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg5D8l5PKZdt"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8Ft3-avJ2-B"
      },
      "outputs": [],
      "source": [
        "# single bit flip을 일으킬 모델 만들기\n",
        "base_fi_model = single_bit_flip_func(\n",
        "    model = copy.deepcopy(model),\n",
        "    batch_size = batch_size, \n",
        "    input_shape = [channels, img_size, img_size], \n",
        "    use_gpu = use_gpu,\n",
        "    bits = quant_bits,\n",
        "    layer_types = layer_type#['all']#[torch.nn.MaxPool2d]#\n",
        ")\n",
        "\n",
        "print(base_fi_model.print_pytorchfi_layer_summary())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#layer_type_names = []\n",
        "#for line in base_fi_model.print_pytorchfi_layer_summary().split(\"\\n\"):\n",
        "#  try:\n",
        "#    int(line.strip().split(\" \")[0])\n",
        "#    print(line.strip().split(\" \"))\n",
        "#  except:\n",
        "#    print(\"not a layer\")\n"
      ],
      "metadata": {
        "id": "TyHyKxqAQfOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "iw5GEQi_LLq8"
      },
      "outputs": [],
      "source": [
        "# single bit flip을 수행할 layer 번호 정리\n",
        "if 'all' in layer_nums:\n",
        "    layer_nums = range(base_fi_model.get_total_layers())\n",
        "else:\n",
        "    layer_nums.sort()\n",
        "    while layer_nums and layer_nums[-1] >= base_fi_model.get_total_layers():\n",
        "        layer_nums.pop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEfYiywpM-ew"
      },
      "outputs": [],
      "source": [
        "# 실험 진행\n",
        "results = []\n",
        "e = open('content/drive/MyDrive/')\n",
        "for layer_num in tqdm(layer_nums):\n",
        "    l = open('/content/drive/MyDrive/' + f\"Trial#{exp_num} Layer#{layer_num} \" + save_dir, \"w\")\n",
        "    l.write(model_name + \"\\n\")\n",
        "    l.write(f\"Layer {layer_num}:\\n\\n\")\n",
        "    orig_correct_cnt = 0\n",
        "    orig_corrupt_diff_cnt = 0\n",
        "    imgnum = 0\n",
        "    for images, labels in dataset:\n",
        "\n",
        "\n",
        "        if use_gpu:\n",
        "            images = images.to(device='cuda')\n",
        "\n",
        "        # 원본에 inference 진행\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            orig_output = model(images)\n",
        "\n",
        "        # single bit flip 위치 지정\n",
        "        layer_num_list = []\n",
        "        dim1 = []\n",
        "        dim2 = []\n",
        "        dim3 = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            layer, C, H, W = pytorchfi.neuron_error_models.random_neuron_location(base_fi_model, layer=layer_num)\n",
        "            layer_num_list.append(layer)\n",
        "            dim1.append(C)\n",
        "            dim2.append(H)\n",
        "            dim3.append(W)\n",
        "\n",
        "            l.write(f\"Image#{imgnum} C: {C} H: {H} W: {W}\\n\")\n",
        "            imgnum += 1\n",
        "\n",
        "        # corrupted model 만들기\n",
        "        corrupted_model = base_fi_model.declare_neuron_fault_injection(\n",
        "            batch = [i for i in range(batch_size)],\n",
        "            layer_num = layer_num_list,\n",
        "            dim1 = dim1,\n",
        "            dim2 = dim2,\n",
        "            dim3 = dim3,\n",
        "            function = base_fi_model.single_bit_flip_signed_across_batch\n",
        "        )\n",
        "\n",
        "        # corrupted model에 inference 진행\n",
        "        corrupted_model.eval()\n",
        "        with torch.no_grad():\n",
        "            corrupted_output = corrupted_model(images)\n",
        "\n",
        "        # 결과 정리\n",
        "        original_output = torch.argmax(orig_output, dim=1).cpu().numpy()\n",
        "        corrupted_output = torch.argmax(corrupted_output, dim=1).cpu().numpy()\n",
        "        labels = labels.numpy()\n",
        "\n",
        "        # 결과 비교: 원본이 정답을 맞춘 경우 중 망가진 모델이 틀린 경우를 셈\n",
        "        for i in range(batch_size):\n",
        "\n",
        "            if labels[i] == original_output[i]:\n",
        "                orig_correct_cnt += 1\n",
        "\n",
        "                if original_output[i] != corrupted_output[i]:\n",
        "                    orig_corrupt_diff_cnt += 1\n",
        "\n",
        "    # 결과 저장\n",
        "    result = f'Layer #{layer_num}: {orig_corrupt_diff_cnt} / {orig_correct_cnt} = {orig_corrupt_diff_cnt / orig_correct_cnt * 100:.4f}%'\n",
        "    results.append(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4Ehzl010Q2zX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bedb03ce-c42c-44db-f730-23973c558241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer #0: 286 / 9291 = 3.0782%\n",
            "Layer #1: 284 / 9293 = 3.0561%\n",
            "Layer #2: 284 / 9291 = 3.0567%\n",
            "Layer #3: 284 / 9293 = 3.0561%\n"
          ]
        }
      ],
      "source": [
        "for result in results:\n",
        "    print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZq5n6aoQ5gM"
      },
      "source": [
        "## 결과 파일 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rlt1CZGdQ7DR"
      },
      "outputs": [],
      "source": [
        "f = open('/content/drive/MyDrive/' + save_dir, 'w')\n",
        "\n",
        "f.write(base_fi_model.print_pytorchfi_layer_summary())\n",
        "f.write(f'\\n\\n===== Result =====\\nQuantization bits: {quant_bits}\\nSeed: {seed}\\n')\n",
        "i = 0\n",
        "for result in results:\n",
        "    f.write(inj_neuron[i])\n",
        "    f.write(result + '\\n')\n",
        "    i += 1\n",
        "\n",
        "f.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
