============================ PYTORCHFI INIT SUMMARY ==============================

Layer types allowing injections:
----------------------------------------------------------------------------------
   - all

Model Info:
----------------------------------------------------------------------------------
   - Shape of input into the model: (3 32 32 )
   - Batch Size: 256
   - CUDA Enabled: True

Layer Info:
----------------------------------------------------------------------------------
Layer #       Layer type  Dimensions         Weight Shape         Output Shape
----------------------------------------------------------------------------------
    0         Identity           4       ['No weights']       [1, 3, 32, 32]
    1           Conv2d           4       [192, 3, 3, 3]     [1, 192, 32, 32]
    2      BatchNorm2d           4                [192]     [1, 192, 32, 32]
    3           Conv2d           4      [64, 192, 1, 1]      [1, 64, 32, 32]
    4      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    5           Conv2d           4      [96, 192, 1, 1]      [1, 96, 32, 32]
    6      BatchNorm2d           4                 [96]      [1, 96, 32, 32]
    7           Conv2d           4      [128, 96, 3, 3]     [1, 128, 32, 32]
    8      BatchNorm2d           4                [128]     [1, 128, 32, 32]
    9           Conv2d           4      [16, 192, 1, 1]      [1, 16, 32, 32]
   10      BatchNorm2d           4                 [16]      [1, 16, 32, 32]
   11           Conv2d           4       [32, 16, 3, 3]      [1, 32, 32, 32]
   12      BatchNorm2d           4                 [32]      [1, 32, 32, 32]
   13        MaxPool2d           4       ['No weights']     [1, 192, 32, 32]
   14           Conv2d           4      [32, 192, 1, 1]      [1, 32, 32, 32]
   15      BatchNorm2d           4                 [32]      [1, 32, 32, 32]
   16           Conv2d           4     [128, 256, 1, 1]     [1, 128, 32, 32]
   17      BatchNorm2d           4                [128]     [1, 128, 32, 32]
   18           Conv2d           4     [128, 256, 1, 1]     [1, 128, 32, 32]
   19      BatchNorm2d           4                [128]     [1, 128, 32, 32]
   20           Conv2d           4     [192, 128, 3, 3]     [1, 192, 32, 32]
   21      BatchNorm2d           4                [192]     [1, 192, 32, 32]
   22           Conv2d           4      [32, 256, 1, 1]      [1, 32, 32, 32]
   23      BatchNorm2d           4                 [32]      [1, 32, 32, 32]
   24           Conv2d           4       [96, 32, 3, 3]      [1, 96, 32, 32]
   25      BatchNorm2d           4                 [96]      [1, 96, 32, 32]
   26        MaxPool2d           4       ['No weights']     [1, 256, 32, 32]
   27           Conv2d           4      [64, 256, 1, 1]      [1, 64, 32, 32]
   28      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
   29        MaxPool2d           4       ['No weights']     [1, 480, 16, 16]
   30           Conv2d           4     [192, 480, 1, 1]     [1, 192, 16, 16]
   31      BatchNorm2d           4                [192]     [1, 192, 16, 16]
   32           Conv2d           4      [96, 480, 1, 1]      [1, 96, 16, 16]
   33      BatchNorm2d           4                 [96]      [1, 96, 16, 16]
   34           Conv2d           4      [208, 96, 3, 3]     [1, 208, 16, 16]
   35      BatchNorm2d           4                [208]     [1, 208, 16, 16]
   36           Conv2d           4      [16, 480, 1, 1]      [1, 16, 16, 16]
   37      BatchNorm2d           4                 [16]      [1, 16, 16, 16]
   38           Conv2d           4       [48, 16, 3, 3]      [1, 48, 16, 16]
   39      BatchNorm2d           4                 [48]      [1, 48, 16, 16]
   40        MaxPool2d           4       ['No weights']     [1, 480, 16, 16]
   41           Conv2d           4      [64, 480, 1, 1]      [1, 64, 16, 16]
   42      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   43           Conv2d           4     [160, 512, 1, 1]     [1, 160, 16, 16]
   44      BatchNorm2d           4                [160]     [1, 160, 16, 16]
   45           Conv2d           4     [112, 512, 1, 1]     [1, 112, 16, 16]
   46      BatchNorm2d           4                [112]     [1, 112, 16, 16]
   47           Conv2d           4     [224, 112, 3, 3]     [1, 224, 16, 16]
   48      BatchNorm2d           4                [224]     [1, 224, 16, 16]
   49           Conv2d           4      [24, 512, 1, 1]      [1, 24, 16, 16]
   50      BatchNorm2d           4                 [24]      [1, 24, 16, 16]
   51           Conv2d           4       [64, 24, 3, 3]      [1, 64, 16, 16]
   52      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   53        MaxPool2d           4       ['No weights']     [1, 512, 16, 16]
   54           Conv2d           4      [64, 512, 1, 1]      [1, 64, 16, 16]
   55      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   56           Conv2d           4     [128, 512, 1, 1]     [1, 128, 16, 16]
   57      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   58           Conv2d           4     [128, 512, 1, 1]     [1, 128, 16, 16]
   59      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   60           Conv2d           4     [256, 128, 3, 3]     [1, 256, 16, 16]
   61      BatchNorm2d           4                [256]     [1, 256, 16, 16]
   62           Conv2d           4      [24, 512, 1, 1]      [1, 24, 16, 16]
   63      BatchNorm2d           4                 [24]      [1, 24, 16, 16]
   64           Conv2d           4       [64, 24, 3, 3]      [1, 64, 16, 16]
   65      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   66        MaxPool2d           4       ['No weights']     [1, 512, 16, 16]
   67           Conv2d           4      [64, 512, 1, 1]      [1, 64, 16, 16]
   68      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   69           Conv2d           4     [112, 512, 1, 1]     [1, 112, 16, 16]
   70      BatchNorm2d           4                [112]     [1, 112, 16, 16]
   71           Conv2d           4     [144, 512, 1, 1]     [1, 144, 16, 16]
   72      BatchNorm2d           4                [144]     [1, 144, 16, 16]
   73           Conv2d           4     [288, 144, 3, 3]     [1, 288, 16, 16]
   74      BatchNorm2d           4                [288]     [1, 288, 16, 16]
   75           Conv2d           4      [32, 512, 1, 1]      [1, 32, 16, 16]
   76      BatchNorm2d           4                 [32]      [1, 32, 16, 16]
   77           Conv2d           4       [64, 32, 3, 3]      [1, 64, 16, 16]
   78      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   79        MaxPool2d           4       ['No weights']     [1, 512, 16, 16]
   80           Conv2d           4      [64, 512, 1, 1]      [1, 64, 16, 16]
   81      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   82           Conv2d           4     [256, 528, 1, 1]     [1, 256, 16, 16]
   83      BatchNorm2d           4                [256]     [1, 256, 16, 16]
   84           Conv2d           4     [160, 528, 1, 1]     [1, 160, 16, 16]
   85      BatchNorm2d           4                [160]     [1, 160, 16, 16]
   86           Conv2d           4     [320, 160, 3, 3]     [1, 320, 16, 16]
   87      BatchNorm2d           4                [320]     [1, 320, 16, 16]
   88           Conv2d           4      [32, 528, 1, 1]      [1, 32, 16, 16]
   89      BatchNorm2d           4                 [32]      [1, 32, 16, 16]
   90           Conv2d           4      [128, 32, 3, 3]     [1, 128, 16, 16]
   91      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   92        MaxPool2d           4       ['No weights']     [1, 528, 16, 16]
   93           Conv2d           4     [128, 528, 1, 1]     [1, 128, 16, 16]
   94      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   95        MaxPool2d           4       ['No weights']       [1, 832, 8, 8]
   96           Conv2d           4     [256, 832, 1, 1]       [1, 256, 8, 8]
   97      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   98           Conv2d           4     [160, 832, 1, 1]       [1, 160, 8, 8]
   99      BatchNorm2d           4                [160]       [1, 160, 8, 8]
  100           Conv2d           4     [320, 160, 3, 3]       [1, 320, 8, 8]
  101      BatchNorm2d           4                [320]       [1, 320, 8, 8]
  102           Conv2d           4      [32, 832, 1, 1]        [1, 32, 8, 8]
  103      BatchNorm2d           4                 [32]        [1, 32, 8, 8]
  104           Conv2d           4      [128, 32, 3, 3]       [1, 128, 8, 8]
  105      BatchNorm2d           4                [128]       [1, 128, 8, 8]
  106        MaxPool2d           4       ['No weights']       [1, 832, 8, 8]
  107           Conv2d           4     [128, 832, 1, 1]       [1, 128, 8, 8]
  108      BatchNorm2d           4                [128]       [1, 128, 8, 8]
  109           Conv2d           4     [384, 832, 1, 1]       [1, 384, 8, 8]
  110      BatchNorm2d           4                [384]       [1, 384, 8, 8]
  111           Conv2d           4     [192, 832, 1, 1]       [1, 192, 8, 8]
  112      BatchNorm2d           4                [192]       [1, 192, 8, 8]
  113           Conv2d           4     [384, 192, 3, 3]       [1, 384, 8, 8]
  114      BatchNorm2d           4                [384]       [1, 384, 8, 8]
  115           Conv2d           4      [48, 832, 1, 1]        [1, 48, 8, 8]
  116      BatchNorm2d           4                 [48]        [1, 48, 8, 8]
  117           Conv2d           4      [128, 48, 3, 3]       [1, 128, 8, 8]
  118      BatchNorm2d           4                [128]       [1, 128, 8, 8]
  119        MaxPool2d           4       ['No weights']       [1, 832, 8, 8]
  120           Conv2d           4     [128, 832, 1, 1]       [1, 128, 8, 8]
  121      BatchNorm2d           4                [128]       [1, 128, 8, 8]
  122  AdaptiveAvgPool2d           4       ['No weights']      [1, 1024, 1, 1]
  123          Dropout           2       ['No weights']            [1, 1024]
  124           Linear           2           [10, 1024]              [1, 10]
==================================================================================


===== Result =====
Quantization bits: 32
Layer #0: 266 / 9269 = 2.8698%
Layer #1: 267 / 9270 = 2.8803%
Layer #2: 266 / 9272 = 2.8689%
Layer #3: 266 / 9270 = 2.8695%
Layer #4: 267 / 9270 = 2.8803%
Layer #5: 267 / 9271 = 2.8799%
Layer #6: 267 / 9272 = 2.8796%
Layer #7: 267 / 9270 = 2.8803%
Layer #8: 266 / 9270 = 2.8695%
Layer #9: 267 / 9270 = 2.8803%
Layer #10: 266 / 9270 = 2.8695%
Layer #11: 267 / 9270 = 2.8803%
Layer #12: 266 / 9272 = 2.8689%
Layer #13: 267 / 9270 = 2.8803%
Layer #14: 266 / 9271 = 2.8692%
Layer #15: 267 / 9270 = 2.8803%
Layer #16: 266 / 9270 = 2.8695%
Layer #17: 267 / 9270 = 2.8803%
Layer #18: 267 / 9269 = 2.8806%
Layer #19: 267 / 9269 = 2.8806%
Layer #20: 267 / 9270 = 2.8803%
Layer #21: 264 / 9272 = 2.8473%
Layer #22: 268 / 9270 = 2.8910%
Layer #23: 267 / 9270 = 2.8803%
Layer #24: 266 / 9271 = 2.8692%
Layer #25: 267 / 9271 = 2.8799%
Layer #26: 266 / 9269 = 2.8698%
Layer #27: 268 / 9271 = 2.8907%
Layer #28: 266 / 9270 = 2.8695%
Layer #29: 267 / 9270 = 2.8803%
Layer #30: 267 / 9270 = 2.8803%
Layer #31: 267 / 9271 = 2.8799%
Layer #32: 267 / 9269 = 2.8806%
Layer #33: 266 / 9269 = 2.8698%
Layer #34: 267 / 9270 = 2.8803%
Layer #35: 266 / 9270 = 2.8695%
Layer #36: 266 / 9269 = 2.8698%
Layer #37: 266 / 9270 = 2.8695%
Layer #38: 267 / 9269 = 2.8806%
Layer #39: 267 / 9269 = 2.8806%
Layer #40: 267 / 9272 = 2.8796%
Layer #41: 268 / 9270 = 2.8910%
Layer #42: 266 / 9271 = 2.8692%
Layer #43: 267 / 9271 = 2.8799%
Layer #44: 265 / 9269 = 2.8590%
Layer #45: 268 / 9269 = 2.8914%
Layer #46: 265 / 9271 = 2.8584%
Layer #47: 266 / 9271 = 2.8692%
Layer #48: 267 / 9270 = 2.8803%
Layer #49: 267 / 9271 = 2.8799%
Layer #50: 267 / 9271 = 2.8799%
Layer #51: 266 / 9270 = 2.8695%
Layer #52: 266 / 9269 = 2.8698%
Layer #53: 267 / 9269 = 2.8806%
Layer #54: 266 / 9270 = 2.8695%
Layer #55: 267 / 9269 = 2.8806%
Layer #56: 267 / 9270 = 2.8803%
Layer #57: 266 / 9274 = 2.8682%
Layer #58: 267 / 9269 = 2.8806%
Layer #59: 266 / 9269 = 2.8698%
Layer #60: 267 / 9269 = 2.8806%
Layer #61: 267 / 9269 = 2.8806%
Layer #62: 267 / 9272 = 2.8796%
Layer #63: 267 / 9269 = 2.8806%
Layer #64: 265 / 9269 = 2.8590%
Layer #65: 267 / 9270 = 2.8803%
Layer #66: 267 / 9270 = 2.8803%
Layer #67: 272 / 9270 = 2.9342%
Layer #68: 268 / 9270 = 2.8910%
Layer #69: 267 / 9269 = 2.8806%
Layer #70: 266 / 9270 = 2.8695%
Layer #71: 265 / 9269 = 2.8590%
Layer #72: 267 / 9270 = 2.8803%
Layer #73: 267 / 9269 = 2.8806%
Layer #74: 266 / 9271 = 2.8692%
Layer #75: 264 / 9270 = 2.8479%
Layer #76: 267 / 9270 = 2.8803%
Layer #77: 267 / 9270 = 2.8803%
Layer #78: 267 / 9270 = 2.8803%
Layer #79: 266 / 9271 = 2.8692%
Layer #80: 266 / 9269 = 2.8698%
Layer #81: 266 / 9271 = 2.8692%
Layer #82: 265 / 9271 = 2.8584%
Layer #83: 267 / 9269 = 2.8806%
Layer #84: 266 / 9270 = 2.8695%
Layer #85: 267 / 9271 = 2.8799%
Layer #86: 268 / 9270 = 2.8910%
Layer #87: 267 / 9270 = 2.8803%
Layer #88: 267 / 9270 = 2.8803%
Layer #89: 266 / 9270 = 2.8695%
Layer #90: 267 / 9270 = 2.8803%
Layer #91: 267 / 9270 = 2.8803%
Layer #92: 267 / 9269 = 2.8806%
Layer #93: 267 / 9273 = 2.8793%
Layer #94: 264 / 9270 = 2.8479%
Layer #95: 268 / 9269 = 2.8914%
Layer #96: 266 / 9271 = 2.8692%
Layer #97: 267 / 9270 = 2.8803%
Layer #98: 267 / 9270 = 2.8803%
Layer #99: 267 / 9269 = 2.8806%
Layer #100: 266 / 9270 = 2.8695%
Layer #101: 267 / 9273 = 2.8793%
Layer #102: 267 / 9269 = 2.8806%
Layer #103: 267 / 9270 = 2.8803%
Layer #104: 267 / 9270 = 2.8803%
Layer #105: 267 / 9269 = 2.8806%
Layer #106: 266 / 9270 = 2.8695%
Layer #107: 267 / 9269 = 2.8806%
Layer #108: 267 / 9269 = 2.8806%
Layer #109: 267 / 9269 = 2.8806%
Layer #110: 267 / 9270 = 2.8803%
Layer #111: 267 / 9273 = 2.8793%
Layer #112: 267 / 9269 = 2.8806%
Layer #113: 266 / 9270 = 2.8695%
Layer #114: 267 / 9270 = 2.8803%
Layer #115: 267 / 9269 = 2.8806%
Layer #116: 267 / 9270 = 2.8803%
Layer #117: 267 / 9269 = 2.8806%
Layer #118: 266 / 9270 = 2.8695%
Layer #119: 267 / 9269 = 2.8806%
Layer #120: 265 / 9270 = 2.8587%
Layer #121: 266 / 9269 = 2.8698%
Layer #122: 267 / 9270 = 2.8803%
Layer #123: 267 / 9269 = 2.8806%
Layer #124: 503 / 9271 = 5.4255%
