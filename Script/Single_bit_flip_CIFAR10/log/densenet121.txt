============================ PYTORCHFI INIT SUMMARY ==============================

Layer types allowing injections:
----------------------------------------------------------------------------------
   - all

Model Info:
----------------------------------------------------------------------------------
   - Shape of input into the model: (3 32 32 )
   - Batch Size: 256
   - CUDA Enabled: True

Layer Info:
----------------------------------------------------------------------------------
Layer #       Layer type  Dimensions         Weight Shape         Output Shape
----------------------------------------------------------------------------------
    0         Identity           4       ['No weights']       [1, 3, 32, 32]
    1           Conv2d           4        [64, 3, 3, 3]      [1, 64, 32, 32]
    2      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    3             ReLU           4       ['No weights']      [1, 64, 32, 32]
    4        MaxPool2d           4       ['No weights']      [1, 64, 16, 16]
    5      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
    6             ReLU           4       ['No weights']      [1, 64, 16, 16]
    7           Conv2d           4      [128, 64, 1, 1]     [1, 128, 16, 16]
    8      BatchNorm2d           4                [128]     [1, 128, 16, 16]
    9             ReLU           4       ['No weights']     [1, 128, 16, 16]
   10           Conv2d           4      [32, 128, 3, 3]      [1, 32, 16, 16]
   11      BatchNorm2d           4                 [96]      [1, 96, 16, 16]
   12             ReLU           4       ['No weights']      [1, 96, 16, 16]
   13           Conv2d           4      [128, 96, 1, 1]     [1, 128, 16, 16]
   14      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   15             ReLU           4       ['No weights']     [1, 128, 16, 16]
   16           Conv2d           4      [32, 128, 3, 3]      [1, 32, 16, 16]
   17      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   18             ReLU           4       ['No weights']     [1, 128, 16, 16]
   19           Conv2d           4     [128, 128, 1, 1]     [1, 128, 16, 16]
   20      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   21             ReLU           4       ['No weights']     [1, 128, 16, 16]
   22           Conv2d           4      [32, 128, 3, 3]      [1, 32, 16, 16]
   23      BatchNorm2d           4                [160]     [1, 160, 16, 16]
   24             ReLU           4       ['No weights']     [1, 160, 16, 16]
   25           Conv2d           4     [128, 160, 1, 1]     [1, 128, 16, 16]
   26      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   27             ReLU           4       ['No weights']     [1, 128, 16, 16]
   28           Conv2d           4      [32, 128, 3, 3]      [1, 32, 16, 16]
   29      BatchNorm2d           4                [192]     [1, 192, 16, 16]
   30             ReLU           4       ['No weights']     [1, 192, 16, 16]
   31           Conv2d           4     [128, 192, 1, 1]     [1, 128, 16, 16]
   32      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   33             ReLU           4       ['No weights']     [1, 128, 16, 16]
   34           Conv2d           4      [32, 128, 3, 3]      [1, 32, 16, 16]
   35      BatchNorm2d           4                [224]     [1, 224, 16, 16]
   36             ReLU           4       ['No weights']     [1, 224, 16, 16]
   37           Conv2d           4     [128, 224, 1, 1]     [1, 128, 16, 16]
   38      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   39             ReLU           4       ['No weights']     [1, 128, 16, 16]
   40           Conv2d           4      [32, 128, 3, 3]      [1, 32, 16, 16]
   41      BatchNorm2d           4                [256]     [1, 256, 16, 16]
   42             ReLU           4       ['No weights']     [1, 256, 16, 16]
   43           Conv2d           4     [128, 256, 1, 1]     [1, 128, 16, 16]
   44        AvgPool2d           4       ['No weights']       [1, 128, 8, 8]
   45      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   46             ReLU           4       ['No weights']       [1, 128, 8, 8]
   47           Conv2d           4     [128, 128, 1, 1]       [1, 128, 8, 8]
   48      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   49             ReLU           4       ['No weights']       [1, 128, 8, 8]
   50           Conv2d           4      [32, 128, 3, 3]        [1, 32, 8, 8]
   51      BatchNorm2d           4                [160]       [1, 160, 8, 8]
   52             ReLU           4       ['No weights']       [1, 160, 8, 8]
   53           Conv2d           4     [128, 160, 1, 1]       [1, 128, 8, 8]
   54      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   55             ReLU           4       ['No weights']       [1, 128, 8, 8]
   56           Conv2d           4      [32, 128, 3, 3]        [1, 32, 8, 8]
   57      BatchNorm2d           4                [192]       [1, 192, 8, 8]
   58             ReLU           4       ['No weights']       [1, 192, 8, 8]
   59           Conv2d           4     [128, 192, 1, 1]       [1, 128, 8, 8]
   60      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   61             ReLU           4       ['No weights']       [1, 128, 8, 8]
   62           Conv2d           4      [32, 128, 3, 3]        [1, 32, 8, 8]
   63      BatchNorm2d           4                [224]       [1, 224, 8, 8]
   64             ReLU           4       ['No weights']       [1, 224, 8, 8]
   65           Conv2d           4     [128, 224, 1, 1]       [1, 128, 8, 8]
   66      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   67             ReLU           4       ['No weights']       [1, 128, 8, 8]
   68           Conv2d           4      [32, 128, 3, 3]        [1, 32, 8, 8]
   69      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   70             ReLU           4       ['No weights']       [1, 256, 8, 8]
   71           Conv2d           4     [128, 256, 1, 1]       [1, 128, 8, 8]
   72      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   73             ReLU           4       ['No weights']       [1, 128, 8, 8]
   74           Conv2d           4      [32, 128, 3, 3]        [1, 32, 8, 8]
   75      BatchNorm2d           4                [288]       [1, 288, 8, 8]
   76             ReLU           4       ['No weights']       [1, 288, 8, 8]
   77           Conv2d           4     [128, 288, 1, 1]       [1, 128, 8, 8]
   78      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   79             ReLU           4       ['No weights']       [1, 128, 8, 8]
   80           Conv2d           4      [32, 128, 3, 3]        [1, 32, 8, 8]
   81      BatchNorm2d           4                [320]       [1, 320, 8, 8]
   82             ReLU           4       ['No weights']       [1, 320, 8, 8]
   83           Conv2d           4     [128, 320, 1, 1]       [1, 128, 8, 8]
   84      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   85             ReLU           4       ['No weights']       [1, 128, 8, 8]
   86           Conv2d           4      [32, 128, 3, 3]        [1, 32, 8, 8]
   87      BatchNorm2d           4                [352]       [1, 352, 8, 8]
   88             ReLU           4       ['No weights']       [1, 352, 8, 8]
   89           Conv2d           4     [128, 352, 1, 1]       [1, 128, 8, 8]
   90      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   91             ReLU           4       ['No weights']       [1, 128, 8, 8]
   92           Conv2d           4      [32, 128, 3, 3]        [1, 32, 8, 8]
   93      BatchNorm2d           4                [384]       [1, 384, 8, 8]
   94             ReLU           4       ['No weights']       [1, 384, 8, 8]
   95           Conv2d           4     [128, 384, 1, 1]       [1, 128, 8, 8]
   96      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   97             ReLU           4       ['No weights']       [1, 128, 8, 8]
   98           Conv2d           4      [32, 128, 3, 3]        [1, 32, 8, 8]
   99      BatchNorm2d           4                [416]       [1, 416, 8, 8]
  100             ReLU           4       ['No weights']       [1, 416, 8, 8]
  101           Conv2d           4     [128, 416, 1, 1]       [1, 128, 8, 8]
  102      BatchNorm2d           4                [128]       [1, 128, 8, 8]
  103             ReLU           4       ['No weights']       [1, 128, 8, 8]
  104           Conv2d           4      [32, 128, 3, 3]        [1, 32, 8, 8]
  105      BatchNorm2d           4                [448]       [1, 448, 8, 8]
  106             ReLU           4       ['No weights']       [1, 448, 8, 8]
  107           Conv2d           4     [128, 448, 1, 1]       [1, 128, 8, 8]
  108      BatchNorm2d           4                [128]       [1, 128, 8, 8]
  109             ReLU           4       ['No weights']       [1, 128, 8, 8]
  110           Conv2d           4      [32, 128, 3, 3]        [1, 32, 8, 8]
  111      BatchNorm2d           4                [480]       [1, 480, 8, 8]
  112             ReLU           4       ['No weights']       [1, 480, 8, 8]
  113           Conv2d           4     [128, 480, 1, 1]       [1, 128, 8, 8]
  114      BatchNorm2d           4                [128]       [1, 128, 8, 8]
  115             ReLU           4       ['No weights']       [1, 128, 8, 8]
  116           Conv2d           4      [32, 128, 3, 3]        [1, 32, 8, 8]
  117      BatchNorm2d           4                [512]       [1, 512, 8, 8]
  118             ReLU           4       ['No weights']       [1, 512, 8, 8]
  119           Conv2d           4     [256, 512, 1, 1]       [1, 256, 8, 8]
  120        AvgPool2d           4       ['No weights']       [1, 256, 4, 4]
  121      BatchNorm2d           4                [256]       [1, 256, 4, 4]
  122             ReLU           4       ['No weights']       [1, 256, 4, 4]
  123           Conv2d           4     [128, 256, 1, 1]       [1, 128, 4, 4]
  124      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  125             ReLU           4       ['No weights']       [1, 128, 4, 4]
  126           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  127      BatchNorm2d           4                [288]       [1, 288, 4, 4]
  128             ReLU           4       ['No weights']       [1, 288, 4, 4]
  129           Conv2d           4     [128, 288, 1, 1]       [1, 128, 4, 4]
  130      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  131             ReLU           4       ['No weights']       [1, 128, 4, 4]
  132           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  133      BatchNorm2d           4                [320]       [1, 320, 4, 4]
  134             ReLU           4       ['No weights']       [1, 320, 4, 4]
  135           Conv2d           4     [128, 320, 1, 1]       [1, 128, 4, 4]
  136      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  137             ReLU           4       ['No weights']       [1, 128, 4, 4]
  138           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  139      BatchNorm2d           4                [352]       [1, 352, 4, 4]
  140             ReLU           4       ['No weights']       [1, 352, 4, 4]
  141           Conv2d           4     [128, 352, 1, 1]       [1, 128, 4, 4]
  142      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  143             ReLU           4       ['No weights']       [1, 128, 4, 4]
  144           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  145      BatchNorm2d           4                [384]       [1, 384, 4, 4]
  146             ReLU           4       ['No weights']       [1, 384, 4, 4]
  147           Conv2d           4     [128, 384, 1, 1]       [1, 128, 4, 4]
  148      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  149             ReLU           4       ['No weights']       [1, 128, 4, 4]
  150           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  151      BatchNorm2d           4                [416]       [1, 416, 4, 4]
  152             ReLU           4       ['No weights']       [1, 416, 4, 4]
  153           Conv2d           4     [128, 416, 1, 1]       [1, 128, 4, 4]
  154      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  155             ReLU           4       ['No weights']       [1, 128, 4, 4]
  156           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  157      BatchNorm2d           4                [448]       [1, 448, 4, 4]
  158             ReLU           4       ['No weights']       [1, 448, 4, 4]
  159           Conv2d           4     [128, 448, 1, 1]       [1, 128, 4, 4]
  160      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  161             ReLU           4       ['No weights']       [1, 128, 4, 4]
  162           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  163      BatchNorm2d           4                [480]       [1, 480, 4, 4]
  164             ReLU           4       ['No weights']       [1, 480, 4, 4]
  165           Conv2d           4     [128, 480, 1, 1]       [1, 128, 4, 4]
  166      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  167             ReLU           4       ['No weights']       [1, 128, 4, 4]
  168           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  169      BatchNorm2d           4                [512]       [1, 512, 4, 4]
  170             ReLU           4       ['No weights']       [1, 512, 4, 4]
  171           Conv2d           4     [128, 512, 1, 1]       [1, 128, 4, 4]
  172      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  173             ReLU           4       ['No weights']       [1, 128, 4, 4]
  174           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  175      BatchNorm2d           4                [544]       [1, 544, 4, 4]
  176             ReLU           4       ['No weights']       [1, 544, 4, 4]
  177           Conv2d           4     [128, 544, 1, 1]       [1, 128, 4, 4]
  178      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  179             ReLU           4       ['No weights']       [1, 128, 4, 4]
  180           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  181      BatchNorm2d           4                [576]       [1, 576, 4, 4]
  182             ReLU           4       ['No weights']       [1, 576, 4, 4]
  183           Conv2d           4     [128, 576, 1, 1]       [1, 128, 4, 4]
  184      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  185             ReLU           4       ['No weights']       [1, 128, 4, 4]
  186           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  187      BatchNorm2d           4                [608]       [1, 608, 4, 4]
  188             ReLU           4       ['No weights']       [1, 608, 4, 4]
  189           Conv2d           4     [128, 608, 1, 1]       [1, 128, 4, 4]
  190      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  191             ReLU           4       ['No weights']       [1, 128, 4, 4]
  192           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  193      BatchNorm2d           4                [640]       [1, 640, 4, 4]
  194             ReLU           4       ['No weights']       [1, 640, 4, 4]
  195           Conv2d           4     [128, 640, 1, 1]       [1, 128, 4, 4]
  196      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  197             ReLU           4       ['No weights']       [1, 128, 4, 4]
  198           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  199      BatchNorm2d           4                [672]       [1, 672, 4, 4]
  200             ReLU           4       ['No weights']       [1, 672, 4, 4]
  201           Conv2d           4     [128, 672, 1, 1]       [1, 128, 4, 4]
  202      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  203             ReLU           4       ['No weights']       [1, 128, 4, 4]
  204           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  205      BatchNorm2d           4                [704]       [1, 704, 4, 4]
  206             ReLU           4       ['No weights']       [1, 704, 4, 4]
  207           Conv2d           4     [128, 704, 1, 1]       [1, 128, 4, 4]
  208      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  209             ReLU           4       ['No weights']       [1, 128, 4, 4]
  210           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  211      BatchNorm2d           4                [736]       [1, 736, 4, 4]
  212             ReLU           4       ['No weights']       [1, 736, 4, 4]
  213           Conv2d           4     [128, 736, 1, 1]       [1, 128, 4, 4]
  214      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  215             ReLU           4       ['No weights']       [1, 128, 4, 4]
  216           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  217      BatchNorm2d           4                [768]       [1, 768, 4, 4]
  218             ReLU           4       ['No weights']       [1, 768, 4, 4]
  219           Conv2d           4     [128, 768, 1, 1]       [1, 128, 4, 4]
  220      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  221             ReLU           4       ['No weights']       [1, 128, 4, 4]
  222           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  223      BatchNorm2d           4                [800]       [1, 800, 4, 4]
  224             ReLU           4       ['No weights']       [1, 800, 4, 4]
  225           Conv2d           4     [128, 800, 1, 1]       [1, 128, 4, 4]
  226      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  227             ReLU           4       ['No weights']       [1, 128, 4, 4]
  228           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  229      BatchNorm2d           4                [832]       [1, 832, 4, 4]
  230             ReLU           4       ['No weights']       [1, 832, 4, 4]
  231           Conv2d           4     [128, 832, 1, 1]       [1, 128, 4, 4]
  232      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  233             ReLU           4       ['No weights']       [1, 128, 4, 4]
  234           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  235      BatchNorm2d           4                [864]       [1, 864, 4, 4]
  236             ReLU           4       ['No weights']       [1, 864, 4, 4]
  237           Conv2d           4     [128, 864, 1, 1]       [1, 128, 4, 4]
  238      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  239             ReLU           4       ['No weights']       [1, 128, 4, 4]
  240           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  241      BatchNorm2d           4                [896]       [1, 896, 4, 4]
  242             ReLU           4       ['No weights']       [1, 896, 4, 4]
  243           Conv2d           4     [128, 896, 1, 1]       [1, 128, 4, 4]
  244      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  245             ReLU           4       ['No weights']       [1, 128, 4, 4]
  246           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  247      BatchNorm2d           4                [928]       [1, 928, 4, 4]
  248             ReLU           4       ['No weights']       [1, 928, 4, 4]
  249           Conv2d           4     [128, 928, 1, 1]       [1, 128, 4, 4]
  250      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  251             ReLU           4       ['No weights']       [1, 128, 4, 4]
  252           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  253      BatchNorm2d           4                [960]       [1, 960, 4, 4]
  254             ReLU           4       ['No weights']       [1, 960, 4, 4]
  255           Conv2d           4     [128, 960, 1, 1]       [1, 128, 4, 4]
  256      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  257             ReLU           4       ['No weights']       [1, 128, 4, 4]
  258           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  259      BatchNorm2d           4                [992]       [1, 992, 4, 4]
  260             ReLU           4       ['No weights']       [1, 992, 4, 4]
  261           Conv2d           4     [128, 992, 1, 1]       [1, 128, 4, 4]
  262      BatchNorm2d           4                [128]       [1, 128, 4, 4]
  263             ReLU           4       ['No weights']       [1, 128, 4, 4]
  264           Conv2d           4      [32, 128, 3, 3]        [1, 32, 4, 4]
  265      BatchNorm2d           4               [1024]      [1, 1024, 4, 4]
  266             ReLU           4       ['No weights']      [1, 1024, 4, 4]
  267           Conv2d           4    [512, 1024, 1, 1]       [1, 512, 4, 4]
  268        AvgPool2d           4       ['No weights']       [1, 512, 2, 2]
  269      BatchNorm2d           4                [512]       [1, 512, 2, 2]
  270             ReLU           4       ['No weights']       [1, 512, 2, 2]
  271           Conv2d           4     [128, 512, 1, 1]       [1, 128, 2, 2]
  272      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  273             ReLU           4       ['No weights']       [1, 128, 2, 2]
  274           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  275      BatchNorm2d           4                [544]       [1, 544, 2, 2]
  276             ReLU           4       ['No weights']       [1, 544, 2, 2]
  277           Conv2d           4     [128, 544, 1, 1]       [1, 128, 2, 2]
  278      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  279             ReLU           4       ['No weights']       [1, 128, 2, 2]
  280           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  281      BatchNorm2d           4                [576]       [1, 576, 2, 2]
  282             ReLU           4       ['No weights']       [1, 576, 2, 2]
  283           Conv2d           4     [128, 576, 1, 1]       [1, 128, 2, 2]
  284      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  285             ReLU           4       ['No weights']       [1, 128, 2, 2]
  286           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  287      BatchNorm2d           4                [608]       [1, 608, 2, 2]
  288             ReLU           4       ['No weights']       [1, 608, 2, 2]
  289           Conv2d           4     [128, 608, 1, 1]       [1, 128, 2, 2]
  290      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  291             ReLU           4       ['No weights']       [1, 128, 2, 2]
  292           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  293      BatchNorm2d           4                [640]       [1, 640, 2, 2]
  294             ReLU           4       ['No weights']       [1, 640, 2, 2]
  295           Conv2d           4     [128, 640, 1, 1]       [1, 128, 2, 2]
  296      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  297             ReLU           4       ['No weights']       [1, 128, 2, 2]
  298           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  299      BatchNorm2d           4                [672]       [1, 672, 2, 2]
  300             ReLU           4       ['No weights']       [1, 672, 2, 2]
  301           Conv2d           4     [128, 672, 1, 1]       [1, 128, 2, 2]
  302      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  303             ReLU           4       ['No weights']       [1, 128, 2, 2]
  304           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  305      BatchNorm2d           4                [704]       [1, 704, 2, 2]
  306             ReLU           4       ['No weights']       [1, 704, 2, 2]
  307           Conv2d           4     [128, 704, 1, 1]       [1, 128, 2, 2]
  308      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  309             ReLU           4       ['No weights']       [1, 128, 2, 2]
  310           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  311      BatchNorm2d           4                [736]       [1, 736, 2, 2]
  312             ReLU           4       ['No weights']       [1, 736, 2, 2]
  313           Conv2d           4     [128, 736, 1, 1]       [1, 128, 2, 2]
  314      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  315             ReLU           4       ['No weights']       [1, 128, 2, 2]
  316           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  317      BatchNorm2d           4                [768]       [1, 768, 2, 2]
  318             ReLU           4       ['No weights']       [1, 768, 2, 2]
  319           Conv2d           4     [128, 768, 1, 1]       [1, 128, 2, 2]
  320      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  321             ReLU           4       ['No weights']       [1, 128, 2, 2]
  322           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  323      BatchNorm2d           4                [800]       [1, 800, 2, 2]
  324             ReLU           4       ['No weights']       [1, 800, 2, 2]
  325           Conv2d           4     [128, 800, 1, 1]       [1, 128, 2, 2]
  326      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  327             ReLU           4       ['No weights']       [1, 128, 2, 2]
  328           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  329      BatchNorm2d           4                [832]       [1, 832, 2, 2]
  330             ReLU           4       ['No weights']       [1, 832, 2, 2]
  331           Conv2d           4     [128, 832, 1, 1]       [1, 128, 2, 2]
  332      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  333             ReLU           4       ['No weights']       [1, 128, 2, 2]
  334           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  335      BatchNorm2d           4                [864]       [1, 864, 2, 2]
  336             ReLU           4       ['No weights']       [1, 864, 2, 2]
  337           Conv2d           4     [128, 864, 1, 1]       [1, 128, 2, 2]
  338      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  339             ReLU           4       ['No weights']       [1, 128, 2, 2]
  340           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  341      BatchNorm2d           4                [896]       [1, 896, 2, 2]
  342             ReLU           4       ['No weights']       [1, 896, 2, 2]
  343           Conv2d           4     [128, 896, 1, 1]       [1, 128, 2, 2]
  344      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  345             ReLU           4       ['No weights']       [1, 128, 2, 2]
  346           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  347      BatchNorm2d           4                [928]       [1, 928, 2, 2]
  348             ReLU           4       ['No weights']       [1, 928, 2, 2]
  349           Conv2d           4     [128, 928, 1, 1]       [1, 128, 2, 2]
  350      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  351             ReLU           4       ['No weights']       [1, 128, 2, 2]
  352           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  353      BatchNorm2d           4                [960]       [1, 960, 2, 2]
  354             ReLU           4       ['No weights']       [1, 960, 2, 2]
  355           Conv2d           4     [128, 960, 1, 1]       [1, 128, 2, 2]
  356      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  357             ReLU           4       ['No weights']       [1, 128, 2, 2]
  358           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  359      BatchNorm2d           4                [992]       [1, 992, 2, 2]
  360             ReLU           4       ['No weights']       [1, 992, 2, 2]
  361           Conv2d           4     [128, 992, 1, 1]       [1, 128, 2, 2]
  362      BatchNorm2d           4                [128]       [1, 128, 2, 2]
  363             ReLU           4       ['No weights']       [1, 128, 2, 2]
  364           Conv2d           4      [32, 128, 3, 3]        [1, 32, 2, 2]
  365      BatchNorm2d           4               [1024]      [1, 1024, 2, 2]
  366           Linear           2           [10, 1024]              [1, 10]
==================================================================================


===== Result =====
Quantization bits: 32
Layer #0: 302 / 9390 = 3.2162%
Layer #1: 301 / 9390 = 3.2055%
Layer #2: 301 / 9393 = 3.2045%
Layer #3: 298 / 9390 = 3.1736%
Layer #4: 299 / 9392 = 3.1836%
Layer #5: 300 / 9391 = 3.1945%
Layer #6: 300 / 9391 = 3.1945%
Layer #7: 298 / 9391 = 3.1733%
Layer #8: 298 / 9391 = 3.1733%
Layer #9: 299 / 9391 = 3.1839%
Layer #10: 301 / 9392 = 3.2049%
Layer #11: 300 / 9392 = 3.1942%
Layer #12: 299 / 9392 = 3.1836%
Layer #13: 298 / 9390 = 3.1736%
Layer #14: 300 / 9393 = 3.1939%
Layer #15: 300 / 9390 = 3.1949%
Layer #16: 300 / 9393 = 3.1939%
Layer #17: 299 / 9391 = 3.1839%
Layer #18: 300 / 9390 = 3.1949%
Layer #19: 300 / 9390 = 3.1949%
Layer #20: 299 / 9390 = 3.1842%
Layer #21: 299 / 9391 = 3.1839%
Layer #22: 301 / 9391 = 3.2052%
Layer #23: 300 / 9392 = 3.1942%
Layer #24: 300 / 9390 = 3.1949%
Layer #25: 299 / 9391 = 3.1839%
Layer #26: 300 / 9390 = 3.1949%
Layer #27: 299 / 9392 = 3.1836%
Layer #28: 298 / 9391 = 3.1733%
Layer #29: 300 / 9391 = 3.1945%
Layer #30: 300 / 9390 = 3.1949%
Layer #31: 300 / 9390 = 3.1949%
Layer #32: 299 / 9390 = 3.1842%
Layer #33: 299 / 9390 = 3.1842%
Layer #34: 300 / 9390 = 3.1949%
Layer #35: 300 / 9392 = 3.1942%
Layer #36: 299 / 9391 = 3.1839%
Layer #37: 300 / 9391 = 3.1945%
Layer #38: 300 / 9391 = 3.1945%
Layer #39: 300 / 9391 = 3.1945%
Layer #40: 299 / 9392 = 3.1836%
Layer #41: 300 / 9391 = 3.1945%
Layer #42: 299 / 9392 = 3.1836%
Layer #43: 300 / 9391 = 3.1945%
Layer #44: 300 / 9390 = 3.1949%
Layer #45: 301 / 9390 = 3.2055%
Layer #46: 300 / 9392 = 3.1942%
Layer #47: 299 / 9392 = 3.1836%
Layer #48: 299 / 9390 = 3.1842%
Layer #49: 301 / 9391 = 3.2052%
Layer #50: 300 / 9390 = 3.1949%
Layer #51: 301 / 9391 = 3.2052%
Layer #52: 300 / 9390 = 3.1949%
Layer #53: 300 / 9392 = 3.1942%
Layer #54: 300 / 9391 = 3.1945%
Layer #55: 300 / 9390 = 3.1949%
Layer #56: 302 / 9390 = 3.2162%
Layer #57: 300 / 9392 = 3.1942%
Layer #58: 300 / 9390 = 3.1949%
Layer #59: 300 / 9391 = 3.1945%
Layer #60: 300 / 9391 = 3.1945%
Layer #61: 300 / 9390 = 3.1949%
Layer #62: 299 / 9392 = 3.1836%
Layer #63: 300 / 9390 = 3.1949%
Layer #64: 299 / 9391 = 3.1839%
Layer #65: 299 / 9393 = 3.1832%
Layer #66: 301 / 9391 = 3.2052%
Layer #67: 300 / 9392 = 3.1942%
Layer #68: 301 / 9391 = 3.2052%
Layer #69: 299 / 9391 = 3.1839%
Layer #70: 300 / 9391 = 3.1945%
Layer #71: 300 / 9391 = 3.1945%
Layer #72: 300 / 9391 = 3.1945%
Layer #73: 299 / 9391 = 3.1839%
Layer #74: 300 / 9393 = 3.1939%
Layer #75: 299 / 9392 = 3.1836%
Layer #76: 300 / 9390 = 3.1949%
Layer #77: 299 / 9390 = 3.1842%
Layer #78: 300 / 9391 = 3.1945%
Layer #79: 300 / 9394 = 3.1935%
Layer #80: 299 / 9390 = 3.1842%
Layer #81: 300 / 9393 = 3.1939%
Layer #82: 300 / 9392 = 3.1942%
Layer #83: 300 / 9390 = 3.1949%
Layer #84: 301 / 9392 = 3.2049%
Layer #85: 300 / 9393 = 3.1939%
Layer #86: 300 / 9390 = 3.1949%
Layer #87: 300 / 9390 = 3.1949%
Layer #88: 301 / 9391 = 3.2052%
Layer #89: 298 / 9391 = 3.1733%
Layer #90: 300 / 9391 = 3.1945%
Layer #91: 299 / 9391 = 3.1839%
Layer #92: 301 / 9390 = 3.2055%
Layer #93: 300 / 9393 = 3.1939%
Layer #94: 300 / 9392 = 3.1942%
Layer #95: 300 / 9391 = 3.1945%
Layer #96: 299 / 9391 = 3.1839%
Layer #97: 300 / 9390 = 3.1949%
Layer #98: 298 / 9391 = 3.1733%
Layer #99: 300 / 9390 = 3.1949%
Layer #100: 299 / 9391 = 3.1839%
Layer #101: 300 / 9394 = 3.1935%
Layer #102: 300 / 9390 = 3.1949%
Layer #103: 300 / 9390 = 3.1949%
Layer #104: 299 / 9391 = 3.1839%
Layer #105: 301 / 9392 = 3.2049%
Layer #106: 301 / 9390 = 3.2055%
Layer #107: 301 / 9391 = 3.2052%
Layer #108: 301 / 9390 = 3.2055%
Layer #109: 299 / 9391 = 3.1839%
Layer #110: 300 / 9390 = 3.1949%
Layer #111: 301 / 9391 = 3.2052%
Layer #112: 300 / 9390 = 3.1949%
Layer #113: 300 / 9391 = 3.1945%
Layer #114: 299 / 9390 = 3.1842%
Layer #115: 302 / 9391 = 3.2158%
Layer #116: 300 / 9392 = 3.1942%
Layer #117: 299 / 9391 = 3.1839%
Layer #118: 301 / 9394 = 3.2042%
Layer #119: 301 / 9391 = 3.2052%
Layer #120: 301 / 9392 = 3.2049%
Layer #121: 299 / 9390 = 3.1842%
Layer #122: 300 / 9391 = 3.1945%
Layer #123: 299 / 9391 = 3.1839%
Layer #124: 300 / 9392 = 3.1942%
Layer #125: 300 / 9392 = 3.1942%
Layer #126: 300 / 9390 = 3.1949%
Layer #127: 299 / 9390 = 3.1842%
Layer #128: 300 / 9391 = 3.1945%
Layer #129: 299 / 9393 = 3.1832%
Layer #130: 300 / 9390 = 3.1949%
Layer #131: 299 / 9390 = 3.1842%
Layer #132: 299 / 9392 = 3.1836%
Layer #133: 298 / 9391 = 3.1733%
Layer #134: 299 / 9390 = 3.1842%
Layer #135: 300 / 9390 = 3.1949%
Layer #136: 300 / 9393 = 3.1939%
Layer #137: 299 / 9391 = 3.1839%
Layer #138: 300 / 9390 = 3.1949%
Layer #139: 299 / 9390 = 3.1842%
Layer #140: 299 / 9391 = 3.1839%
Layer #141: 301 / 9390 = 3.2055%
Layer #142: 301 / 9391 = 3.2052%
Layer #143: 298 / 9390 = 3.1736%
Layer #144: 300 / 9391 = 3.1945%
Layer #145: 300 / 9391 = 3.1945%
Layer #146: 300 / 9392 = 3.1942%
Layer #147: 299 / 9393 = 3.1832%
Layer #148: 298 / 9391 = 3.1733%
Layer #149: 301 / 9391 = 3.2052%
Layer #150: 301 / 9390 = 3.2055%
Layer #151: 298 / 9390 = 3.1736%
Layer #152: 300 / 9393 = 3.1939%
Layer #153: 299 / 9392 = 3.1836%
Layer #154: 300 / 9390 = 3.1949%
Layer #155: 300 / 9391 = 3.1945%
Layer #156: 300 / 9391 = 3.1945%
Layer #157: 300 / 9390 = 3.1949%
Layer #158: 300 / 9390 = 3.1949%
Layer #159: 299 / 9390 = 3.1842%
Layer #160: 298 / 9390 = 3.1736%
Layer #161: 301 / 9390 = 3.2055%
Layer #162: 300 / 9391 = 3.1945%
Layer #163: 300 / 9391 = 3.1945%
Layer #164: 301 / 9391 = 3.2052%
Layer #165: 299 / 9391 = 3.1839%
Layer #166: 299 / 9391 = 3.1839%
Layer #167: 300 / 9390 = 3.1949%
Layer #168: 300 / 9392 = 3.1942%
Layer #169: 299 / 9391 = 3.1839%
Layer #170: 299 / 9391 = 3.1839%
Layer #171: 300 / 9390 = 3.1949%
Layer #172: 300 / 9391 = 3.1945%
Layer #173: 298 / 9391 = 3.1733%
Layer #174: 300 / 9391 = 3.1945%
Layer #175: 300 / 9390 = 3.1949%
Layer #176: 300 / 9390 = 3.1949%
Layer #177: 300 / 9390 = 3.1949%
Layer #178: 300 / 9391 = 3.1945%
Layer #179: 299 / 9393 = 3.1832%
Layer #180: 298 / 9390 = 3.1736%
Layer #181: 299 / 9390 = 3.1842%
Layer #182: 298 / 9390 = 3.1736%
Layer #183: 299 / 9390 = 3.1842%
Layer #184: 300 / 9391 = 3.1945%
Layer #185: 299 / 9390 = 3.1842%
Layer #186: 300 / 9391 = 3.1945%
Layer #187: 300 / 9391 = 3.1945%
Layer #188: 298 / 9391 = 3.1733%
Layer #189: 299 / 9392 = 3.1836%
Layer #190: 301 / 9390 = 3.2055%
Layer #191: 302 / 9390 = 3.2162%
Layer #192: 300 / 9390 = 3.1949%
Layer #193: 299 / 9390 = 3.1842%
Layer #194: 299 / 9391 = 3.1839%
Layer #195: 301 / 9394 = 3.2042%
Layer #196: 300 / 9391 = 3.1945%
Layer #197: 298 / 9391 = 3.1733%
Layer #198: 299 / 9394 = 3.1829%
Layer #199: 299 / 9391 = 3.1839%
Layer #200: 297 / 9391 = 3.1626%
Layer #201: 299 / 9390 = 3.1842%
Layer #202: 298 / 9391 = 3.1733%
Layer #203: 300 / 9390 = 3.1949%
Layer #204: 299 / 9392 = 3.1836%
Layer #205: 300 / 9390 = 3.1949%
Layer #206: 298 / 9392 = 3.1729%
Layer #207: 300 / 9390 = 3.1949%
Layer #208: 299 / 9390 = 3.1842%
Layer #209: 299 / 9391 = 3.1839%
Layer #210: 300 / 9390 = 3.1949%
Layer #211: 299 / 9390 = 3.1842%
Layer #212: 300 / 9391 = 3.1945%
Layer #213: 300 / 9390 = 3.1949%
Layer #214: 299 / 9390 = 3.1842%
Layer #215: 299 / 9392 = 3.1836%
Layer #216: 299 / 9390 = 3.1842%
Layer #217: 300 / 9390 = 3.1949%
Layer #218: 300 / 9391 = 3.1945%
Layer #219: 300 / 9392 = 3.1942%
Layer #220: 298 / 9391 = 3.1733%
Layer #221: 300 / 9391 = 3.1945%
Layer #222: 300 / 9391 = 3.1945%
Layer #223: 300 / 9391 = 3.1945%
Layer #224: 300 / 9390 = 3.1949%
Layer #225: 298 / 9394 = 3.1722%
Layer #226: 300 / 9391 = 3.1945%
Layer #227: 300 / 9393 = 3.1939%
Layer #228: 300 / 9390 = 3.1949%
Layer #229: 298 / 9391 = 3.1733%
Layer #230: 300 / 9392 = 3.1942%
Layer #231: 300 / 9392 = 3.1942%
Layer #232: 300 / 9394 = 3.1935%
Layer #233: 299 / 9390 = 3.1842%
Layer #234: 300 / 9393 = 3.1939%
Layer #235: 300 / 9393 = 3.1939%
Layer #236: 298 / 9392 = 3.1729%
Layer #237: 298 / 9390 = 3.1736%
Layer #238: 299 / 9391 = 3.1839%
Layer #239: 299 / 9391 = 3.1839%
Layer #240: 300 / 9391 = 3.1945%
Layer #241: 300 / 9391 = 3.1945%
Layer #242: 299 / 9391 = 3.1839%
Layer #243: 300 / 9393 = 3.1939%
Layer #244: 299 / 9390 = 3.1842%
Layer #245: 300 / 9390 = 3.1949%
Layer #246: 300 / 9392 = 3.1942%
Layer #247: 300 / 9391 = 3.1945%
Layer #248: 299 / 9391 = 3.1839%
Layer #249: 299 / 9391 = 3.1839%
Layer #250: 300 / 9390 = 3.1949%
Layer #251: 300 / 9392 = 3.1942%
Layer #252: 299 / 9391 = 3.1839%
Layer #253: 300 / 9391 = 3.1945%
Layer #254: 300 / 9391 = 3.1945%
Layer #255: 300 / 9392 = 3.1942%
Layer #256: 300 / 9391 = 3.1945%
Layer #257: 298 / 9391 = 3.1733%
Layer #258: 299 / 9390 = 3.1842%
Layer #259: 300 / 9393 = 3.1939%
Layer #260: 299 / 9390 = 3.1842%
Layer #261: 300 / 9391 = 3.1945%
Layer #262: 299 / 9391 = 3.1839%
Layer #263: 300 / 9390 = 3.1949%
Layer #264: 300 / 9390 = 3.1949%
Layer #265: 300 / 9390 = 3.1949%
Layer #266: 298 / 9391 = 3.1733%
Layer #267: 299 / 9390 = 3.1842%
Layer #268: 299 / 9391 = 3.1839%
Layer #269: 300 / 9390 = 3.1949%
Layer #270: 300 / 9390 = 3.1949%
Layer #271: 299 / 9390 = 3.1842%
Layer #272: 299 / 9391 = 3.1839%
Layer #273: 300 / 9390 = 3.1949%
Layer #274: 300 / 9390 = 3.1949%
Layer #275: 299 / 9390 = 3.1842%
Layer #276: 299 / 9390 = 3.1842%
Layer #277: 299 / 9391 = 3.1839%
Layer #278: 300 / 9391 = 3.1945%
Layer #279: 300 / 9391 = 3.1945%
Layer #280: 300 / 9393 = 3.1939%
Layer #281: 300 / 9391 = 3.1945%
Layer #282: 299 / 9393 = 3.1832%
Layer #283: 299 / 9390 = 3.1842%
Layer #284: 299 / 9390 = 3.1842%
Layer #285: 299 / 9391 = 3.1839%
Layer #286: 299 / 9391 = 3.1839%
Layer #287: 300 / 9392 = 3.1942%
Layer #288: 300 / 9392 = 3.1942%
Layer #289: 299 / 9391 = 3.1839%
Layer #290: 300 / 9392 = 3.1942%
Layer #291: 299 / 9391 = 3.1839%
Layer #292: 300 / 9391 = 3.1945%
Layer #293: 300 / 9394 = 3.1935%
Layer #294: 300 / 9392 = 3.1942%
Layer #295: 300 / 9391 = 3.1945%
Layer #296: 300 / 9390 = 3.1949%
Layer #297: 300 / 9391 = 3.1945%
Layer #298: 300 / 9392 = 3.1942%
Layer #299: 300 / 9390 = 3.1949%
Layer #300: 300 / 9390 = 3.1949%
Layer #301: 299 / 9390 = 3.1842%
Layer #302: 300 / 9390 = 3.1949%
Layer #303: 300 / 9393 = 3.1939%
Layer #304: 299 / 9392 = 3.1836%
Layer #305: 300 / 9394 = 3.1935%
Layer #306: 300 / 9390 = 3.1949%
Layer #307: 300 / 9390 = 3.1949%
Layer #308: 299 / 9391 = 3.1839%
Layer #309: 300 / 9390 = 3.1949%
Layer #310: 300 / 9392 = 3.1942%
Layer #311: 299 / 9391 = 3.1839%
Layer #312: 298 / 9392 = 3.1729%
Layer #313: 299 / 9394 = 3.1829%
Layer #314: 300 / 9391 = 3.1945%
Layer #315: 300 / 9391 = 3.1945%
Layer #316: 299 / 9393 = 3.1832%
Layer #317: 299 / 9391 = 3.1839%
Layer #318: 300 / 9390 = 3.1949%
Layer #319: 300 / 9390 = 3.1949%
Layer #320: 299 / 9390 = 3.1842%
Layer #321: 300 / 9390 = 3.1949%
Layer #322: 299 / 9391 = 3.1839%
Layer #323: 300 / 9391 = 3.1945%
Layer #324: 299 / 9391 = 3.1839%
Layer #325: 300 / 9392 = 3.1942%
Layer #326: 299 / 9390 = 3.1842%
Layer #327: 300 / 9390 = 3.1949%
Layer #328: 300 / 9391 = 3.1945%
Layer #329: 299 / 9391 = 3.1839%
Layer #330: 300 / 9390 = 3.1949%
Layer #331: 300 / 9390 = 3.1949%
Layer #332: 300 / 9390 = 3.1949%
Layer #333: 299 / 9391 = 3.1839%
Layer #334: 300 / 9393 = 3.1939%
Layer #335: 299 / 9390 = 3.1842%
Layer #336: 300 / 9391 = 3.1945%
Layer #337: 298 / 9391 = 3.1733%
Layer #338: 299 / 9391 = 3.1839%
Layer #339: 299 / 9390 = 3.1842%
Layer #340: 300 / 9390 = 3.1949%
Layer #341: 298 / 9390 = 3.1736%
Layer #342: 300 / 9391 = 3.1945%
Layer #343: 299 / 9391 = 3.1839%
Layer #344: 300 / 9392 = 3.1942%
Layer #345: 299 / 9390 = 3.1842%
Layer #346: 299 / 9390 = 3.1842%
Layer #347: 300 / 9392 = 3.1942%
Layer #348: 299 / 9392 = 3.1836%
Layer #349: 299 / 9391 = 3.1839%
Layer #350: 300 / 9390 = 3.1949%
Layer #351: 300 / 9390 = 3.1949%
Layer #352: 300 / 9390 = 3.1949%
Layer #353: 299 / 9391 = 3.1839%
Layer #354: 298 / 9392 = 3.1729%
Layer #355: 300 / 9391 = 3.1945%
Layer #356: 300 / 9391 = 3.1945%
Layer #357: 300 / 9391 = 3.1945%
Layer #358: 300 / 9392 = 3.1942%
Layer #359: 300 / 9390 = 3.1949%
Layer #360: 300 / 9392 = 3.1942%
Layer #361: 300 / 9393 = 3.1939%
Layer #362: 299 / 9390 = 3.1842%
Layer #363: 299 / 9391 = 3.1839%
Layer #364: 300 / 9390 = 3.1949%
Layer #365: 300 / 9390 = 3.1949%
Layer #366: 464 / 9391 = 4.9409%
