============================ PYTORCHFI INIT SUMMARY ==============================

Layer types allowing injections:
----------------------------------------------------------------------------------
   - all

Model Info:
----------------------------------------------------------------------------------
   - Shape of input into the model: (3 32 32 )
   - Batch Size: 256
   - CUDA Enabled: True

Layer Info:
----------------------------------------------------------------------------------
Layer #       Layer type  Dimensions         Weight Shape         Output Shape
----------------------------------------------------------------------------------
    0         Identity           4       ['No weights']       [1, 3, 32, 32]
    1           Conv2d           4        [32, 3, 3, 3]      [1, 32, 32, 32]
    2      BatchNorm2d           4                 [32]      [1, 32, 32, 32]
    3            ReLU6           4       ['No weights']      [1, 32, 32, 32]
    4           Conv2d           4        [32, 1, 3, 3]      [1, 32, 32, 32]
    5      BatchNorm2d           4                 [32]      [1, 32, 32, 32]
    6            ReLU6           4       ['No weights']      [1, 32, 32, 32]
    7           Conv2d           4       [16, 32, 1, 1]      [1, 16, 32, 32]
    8      BatchNorm2d           4                 [16]      [1, 16, 32, 32]
    9           Conv2d           4       [96, 16, 1, 1]      [1, 96, 32, 32]
   10      BatchNorm2d           4                 [96]      [1, 96, 32, 32]
   11            ReLU6           4       ['No weights']      [1, 96, 32, 32]
   12           Conv2d           4        [96, 1, 3, 3]      [1, 96, 32, 32]
   13      BatchNorm2d           4                 [96]      [1, 96, 32, 32]
   14            ReLU6           4       ['No weights']      [1, 96, 32, 32]
   15           Conv2d           4       [24, 96, 1, 1]      [1, 24, 32, 32]
   16      BatchNorm2d           4                 [24]      [1, 24, 32, 32]
   17           Conv2d           4      [144, 24, 1, 1]     [1, 144, 32, 32]
   18      BatchNorm2d           4                [144]     [1, 144, 32, 32]
   19            ReLU6           4       ['No weights']     [1, 144, 32, 32]
   20           Conv2d           4       [144, 1, 3, 3]     [1, 144, 32, 32]
   21      BatchNorm2d           4                [144]     [1, 144, 32, 32]
   22            ReLU6           4       ['No weights']     [1, 144, 32, 32]
   23           Conv2d           4      [24, 144, 1, 1]      [1, 24, 32, 32]
   24      BatchNorm2d           4                 [24]      [1, 24, 32, 32]
   25           Conv2d           4      [144, 24, 1, 1]     [1, 144, 32, 32]
   26      BatchNorm2d           4                [144]     [1, 144, 32, 32]
   27            ReLU6           4       ['No weights']     [1, 144, 32, 32]
   28           Conv2d           4       [144, 1, 3, 3]     [1, 144, 16, 16]
   29      BatchNorm2d           4                [144]     [1, 144, 16, 16]
   30            ReLU6           4       ['No weights']     [1, 144, 16, 16]
   31           Conv2d           4      [32, 144, 1, 1]      [1, 32, 16, 16]
   32      BatchNorm2d           4                 [32]      [1, 32, 16, 16]
   33           Conv2d           4      [192, 32, 1, 1]     [1, 192, 16, 16]
   34      BatchNorm2d           4                [192]     [1, 192, 16, 16]
   35            ReLU6           4       ['No weights']     [1, 192, 16, 16]
   36           Conv2d           4       [192, 1, 3, 3]     [1, 192, 16, 16]
   37      BatchNorm2d           4                [192]     [1, 192, 16, 16]
   38            ReLU6           4       ['No weights']     [1, 192, 16, 16]
   39           Conv2d           4      [32, 192, 1, 1]      [1, 32, 16, 16]
   40      BatchNorm2d           4                 [32]      [1, 32, 16, 16]
   41           Conv2d           4      [192, 32, 1, 1]     [1, 192, 16, 16]
   42      BatchNorm2d           4                [192]     [1, 192, 16, 16]
   43            ReLU6           4       ['No weights']     [1, 192, 16, 16]
   44           Conv2d           4       [192, 1, 3, 3]     [1, 192, 16, 16]
   45      BatchNorm2d           4                [192]     [1, 192, 16, 16]
   46            ReLU6           4       ['No weights']     [1, 192, 16, 16]
   47           Conv2d           4      [32, 192, 1, 1]      [1, 32, 16, 16]
   48      BatchNorm2d           4                 [32]      [1, 32, 16, 16]
   49           Conv2d           4      [192, 32, 1, 1]     [1, 192, 16, 16]
   50      BatchNorm2d           4                [192]     [1, 192, 16, 16]
   51            ReLU6           4       ['No weights']     [1, 192, 16, 16]
   52           Conv2d           4       [192, 1, 3, 3]       [1, 192, 8, 8]
   53      BatchNorm2d           4                [192]       [1, 192, 8, 8]
   54            ReLU6           4       ['No weights']       [1, 192, 8, 8]
   55           Conv2d           4      [64, 192, 1, 1]        [1, 64, 8, 8]
   56      BatchNorm2d           4                 [64]        [1, 64, 8, 8]
   57           Conv2d           4      [384, 64, 1, 1]       [1, 384, 8, 8]
   58      BatchNorm2d           4                [384]       [1, 384, 8, 8]
   59            ReLU6           4       ['No weights']       [1, 384, 8, 8]
   60           Conv2d           4       [384, 1, 3, 3]       [1, 384, 8, 8]
   61      BatchNorm2d           4                [384]       [1, 384, 8, 8]
   62            ReLU6           4       ['No weights']       [1, 384, 8, 8]
   63           Conv2d           4      [64, 384, 1, 1]        [1, 64, 8, 8]
   64      BatchNorm2d           4                 [64]        [1, 64, 8, 8]
   65           Conv2d           4      [384, 64, 1, 1]       [1, 384, 8, 8]
   66      BatchNorm2d           4                [384]       [1, 384, 8, 8]
   67            ReLU6           4       ['No weights']       [1, 384, 8, 8]
   68           Conv2d           4       [384, 1, 3, 3]       [1, 384, 8, 8]
   69      BatchNorm2d           4                [384]       [1, 384, 8, 8]
   70            ReLU6           4       ['No weights']       [1, 384, 8, 8]
   71           Conv2d           4      [64, 384, 1, 1]        [1, 64, 8, 8]
   72      BatchNorm2d           4                 [64]        [1, 64, 8, 8]
   73           Conv2d           4      [384, 64, 1, 1]       [1, 384, 8, 8]
   74      BatchNorm2d           4                [384]       [1, 384, 8, 8]
   75            ReLU6           4       ['No weights']       [1, 384, 8, 8]
   76           Conv2d           4       [384, 1, 3, 3]       [1, 384, 8, 8]
   77      BatchNorm2d           4                [384]       [1, 384, 8, 8]
   78            ReLU6           4       ['No weights']       [1, 384, 8, 8]
   79           Conv2d           4      [64, 384, 1, 1]        [1, 64, 8, 8]
   80      BatchNorm2d           4                 [64]        [1, 64, 8, 8]
   81           Conv2d           4      [384, 64, 1, 1]       [1, 384, 8, 8]
   82      BatchNorm2d           4                [384]       [1, 384, 8, 8]
   83            ReLU6           4       ['No weights']       [1, 384, 8, 8]
   84           Conv2d           4       [384, 1, 3, 3]       [1, 384, 8, 8]
   85      BatchNorm2d           4                [384]       [1, 384, 8, 8]
   86            ReLU6           4       ['No weights']       [1, 384, 8, 8]
   87           Conv2d           4      [96, 384, 1, 1]        [1, 96, 8, 8]
   88      BatchNorm2d           4                 [96]        [1, 96, 8, 8]
   89           Conv2d           4      [576, 96, 1, 1]       [1, 576, 8, 8]
   90      BatchNorm2d           4                [576]       [1, 576, 8, 8]
   91            ReLU6           4       ['No weights']       [1, 576, 8, 8]
   92           Conv2d           4       [576, 1, 3, 3]       [1, 576, 8, 8]
   93      BatchNorm2d           4                [576]       [1, 576, 8, 8]
   94            ReLU6           4       ['No weights']       [1, 576, 8, 8]
   95           Conv2d           4      [96, 576, 1, 1]        [1, 96, 8, 8]
   96      BatchNorm2d           4                 [96]        [1, 96, 8, 8]
   97           Conv2d           4      [576, 96, 1, 1]       [1, 576, 8, 8]
   98      BatchNorm2d           4                [576]       [1, 576, 8, 8]
   99            ReLU6           4       ['No weights']       [1, 576, 8, 8]
  100           Conv2d           4       [576, 1, 3, 3]       [1, 576, 8, 8]
  101      BatchNorm2d           4                [576]       [1, 576, 8, 8]
  102            ReLU6           4       ['No weights']       [1, 576, 8, 8]
  103           Conv2d           4      [96, 576, 1, 1]        [1, 96, 8, 8]
  104      BatchNorm2d           4                 [96]        [1, 96, 8, 8]
  105           Conv2d           4      [576, 96, 1, 1]       [1, 576, 8, 8]
  106      BatchNorm2d           4                [576]       [1, 576, 8, 8]
  107            ReLU6           4       ['No weights']       [1, 576, 8, 8]
  108           Conv2d           4       [576, 1, 3, 3]       [1, 576, 4, 4]
  109      BatchNorm2d           4                [576]       [1, 576, 4, 4]
  110            ReLU6           4       ['No weights']       [1, 576, 4, 4]
  111           Conv2d           4     [160, 576, 1, 1]       [1, 160, 4, 4]
  112      BatchNorm2d           4                [160]       [1, 160, 4, 4]
  113           Conv2d           4     [960, 160, 1, 1]       [1, 960, 4, 4]
  114      BatchNorm2d           4                [960]       [1, 960, 4, 4]
  115            ReLU6           4       ['No weights']       [1, 960, 4, 4]
  116           Conv2d           4       [960, 1, 3, 3]       [1, 960, 4, 4]
  117      BatchNorm2d           4                [960]       [1, 960, 4, 4]
  118            ReLU6           4       ['No weights']       [1, 960, 4, 4]
  119           Conv2d           4     [160, 960, 1, 1]       [1, 160, 4, 4]
  120      BatchNorm2d           4                [160]       [1, 160, 4, 4]
  121           Conv2d           4     [960, 160, 1, 1]       [1, 960, 4, 4]
  122      BatchNorm2d           4                [960]       [1, 960, 4, 4]
  123            ReLU6           4       ['No weights']       [1, 960, 4, 4]
  124           Conv2d           4       [960, 1, 3, 3]       [1, 960, 4, 4]
  125      BatchNorm2d           4                [960]       [1, 960, 4, 4]
  126            ReLU6           4       ['No weights']       [1, 960, 4, 4]
  127           Conv2d           4     [160, 960, 1, 1]       [1, 160, 4, 4]
  128      BatchNorm2d           4                [160]       [1, 160, 4, 4]
  129           Conv2d           4     [960, 160, 1, 1]       [1, 960, 4, 4]
  130      BatchNorm2d           4                [960]       [1, 960, 4, 4]
  131            ReLU6           4       ['No weights']       [1, 960, 4, 4]
  132           Conv2d           4       [960, 1, 3, 3]       [1, 960, 4, 4]
  133      BatchNorm2d           4                [960]       [1, 960, 4, 4]
  134            ReLU6           4       ['No weights']       [1, 960, 4, 4]
  135           Conv2d           4     [320, 960, 1, 1]       [1, 320, 4, 4]
  136      BatchNorm2d           4                [320]       [1, 320, 4, 4]
  137           Conv2d           4    [1280, 320, 1, 1]      [1, 1280, 4, 4]
  138      BatchNorm2d           4               [1280]      [1, 1280, 4, 4]
  139            ReLU6           4       ['No weights']      [1, 1280, 4, 4]
  140          Dropout           2       ['No weights']            [1, 1280]
  141           Linear           2           [10, 1280]              [1, 10]
==================================================================================


===== Result =====
Quantization bits: 32
Seed: 1234
Layer #0: 231 / 9376 = 2.4637%
Layer #1: 227 / 9375 = 2.4213%
Layer #2: 227 / 9376 = 2.4211%
Layer #3: 228 / 9377 = 2.4315%
Layer #4: 227 / 9378 = 2.4206%
Layer #5: 229 / 9375 = 2.4427%
Layer #6: 225 / 9376 = 2.3997%
Layer #7: 225 / 9376 = 2.3997%
Layer #8: 224 / 9376 = 2.3891%
Layer #9: 227 / 9377 = 2.4208%
Layer #10: 228 / 9375 = 2.4320%
Layer #11: 227 / 9376 = 2.4211%
Layer #12: 226 / 9377 = 2.4102%
Layer #13: 228 / 9376 = 2.4317%
Layer #14: 228 / 9377 = 2.4315%
Layer #15: 228 / 9375 = 2.4320%
Layer #16: 228 / 9379 = 2.4310%
Layer #17: 228 / 9376 = 2.4317%
Layer #18: 228 / 9375 = 2.4320%
Layer #19: 228 / 9375 = 2.4320%
Layer #20: 228 / 9375 = 2.4320%
Layer #21: 226 / 9377 = 2.4102%
Layer #22: 228 / 9376 = 2.4317%
Layer #23: 228 / 9376 = 2.4317%
Layer #24: 227 / 9375 = 2.4213%
Layer #25: 228 / 9377 = 2.4315%
Layer #26: 228 / 9375 = 2.4320%
Layer #27: 228 / 9376 = 2.4317%
Layer #28: 228 / 9375 = 2.4320%
Layer #29: 229 / 9376 = 2.4424%
Layer #30: 228 / 9375 = 2.4320%
Layer #31: 228 / 9375 = 2.4320%
Layer #32: 226 / 9375 = 2.4107%
Layer #33: 228 / 9377 = 2.4315%
Layer #34: 227 / 9376 = 2.4211%
Layer #35: 226 / 9375 = 2.4107%
Layer #36: 227 / 9375 = 2.4213%
Layer #37: 228 / 9376 = 2.4317%
Layer #38: 228 / 9376 = 2.4317%
Layer #39: 228 / 9377 = 2.4315%
Layer #40: 229 / 9376 = 2.4424%
Layer #41: 227 / 9375 = 2.4213%
Layer #42: 227 / 9379 = 2.4203%
Layer #43: 228 / 9378 = 2.4312%
Layer #44: 227 / 9375 = 2.4213%
Layer #45: 228 / 9376 = 2.4317%
Layer #46: 227 / 9377 = 2.4208%
Layer #47: 227 / 9377 = 2.4208%
Layer #48: 227 / 9376 = 2.4211%
Layer #49: 228 / 9377 = 2.4315%
Layer #50: 228 / 9376 = 2.4317%
Layer #51: 228 / 9375 = 2.4320%
Layer #52: 224 / 9375 = 2.3893%
Layer #53: 230 / 9375 = 2.4533%
Layer #54: 226 / 9376 = 2.4104%
Layer #55: 228 / 9375 = 2.4320%
Layer #56: 226 / 9375 = 2.4107%
Layer #57: 227 / 9377 = 2.4208%
Layer #58: 227 / 9375 = 2.4213%
Layer #59: 228 / 9375 = 2.4320%
Layer #60: 228 / 9375 = 2.4320%
Layer #61: 229 / 9375 = 2.4427%
Layer #62: 226 / 9377 = 2.4102%
Layer #63: 227 / 9375 = 2.4213%
Layer #64: 227 / 9375 = 2.4213%
Layer #65: 226 / 9375 = 2.4107%
Layer #66: 226 / 9376 = 2.4104%
Layer #67: 228 / 9375 = 2.4320%
Layer #68: 227 / 9375 = 2.4213%
Layer #69: 228 / 9375 = 2.4320%
Layer #70: 227 / 9376 = 2.4211%
Layer #71: 225 / 9375 = 2.4000%
Layer #72: 227 / 9376 = 2.4211%
Layer #73: 227 / 9375 = 2.4213%
Layer #74: 226 / 9377 = 2.4102%
Layer #75: 226 / 9377 = 2.4102%
Layer #76: 228 / 9375 = 2.4320%
Layer #77: 228 / 9376 = 2.4317%
Layer #78: 228 / 9376 = 2.4317%
Layer #79: 226 / 9377 = 2.4102%
Layer #80: 228 / 9376 = 2.4317%
Layer #81: 228 / 9377 = 2.4315%
Layer #82: 230 / 9377 = 2.4528%
Layer #83: 230 / 9375 = 2.4533%
Layer #84: 229 / 9377 = 2.4421%
Layer #85: 227 / 9375 = 2.4213%
Layer #86: 227 / 9375 = 2.4213%
Layer #87: 227 / 9375 = 2.4213%
Layer #88: 227 / 9375 = 2.4213%
Layer #89: 228 / 9376 = 2.4317%
Layer #90: 228 / 9376 = 2.4317%
Layer #91: 228 / 9376 = 2.4317%
Layer #92: 228 / 9375 = 2.4320%
Layer #93: 228 / 9377 = 2.4315%
Layer #94: 228 / 9377 = 2.4315%
Layer #95: 228 / 9375 = 2.4320%
Layer #96: 228 / 9376 = 2.4317%
Layer #97: 226 / 9375 = 2.4107%
Layer #98: 227 / 9378 = 2.4206%
Layer #99: 228 / 9375 = 2.4320%
Layer #100: 228 / 9377 = 2.4315%
Layer #101: 228 / 9378 = 2.4312%
Layer #102: 226 / 9375 = 2.4107%
Layer #103: 228 / 9375 = 2.4320%
Layer #104: 225 / 9376 = 2.3997%
Layer #105: 228 / 9376 = 2.4317%
Layer #106: 228 / 9376 = 2.4317%
Layer #107: 228 / 9376 = 2.4317%
Layer #108: 230 / 9375 = 2.4533%
Layer #109: 228 / 9376 = 2.4317%
Layer #110: 229 / 9375 = 2.4427%
Layer #111: 227 / 9378 = 2.4206%
Layer #112: 227 / 9375 = 2.4213%
Layer #113: 227 / 9375 = 2.4213%
Layer #114: 228 / 9376 = 2.4317%
Layer #115: 228 / 9376 = 2.4317%
Layer #116: 227 / 9377 = 2.4208%
Layer #117: 229 / 9375 = 2.4427%
Layer #118: 227 / 9376 = 2.4211%
Layer #119: 229 / 9375 = 2.4427%
Layer #120: 229 / 9376 = 2.4424%
Layer #121: 228 / 9377 = 2.4315%
Layer #122: 228 / 9376 = 2.4317%
Layer #123: 228 / 9375 = 2.4320%
Layer #124: 227 / 9377 = 2.4208%
Layer #125: 228 / 9377 = 2.4315%
Layer #126: 227 / 9376 = 2.4211%
Layer #127: 229 / 9375 = 2.4427%
Layer #128: 228 / 9377 = 2.4315%
Layer #129: 228 / 9378 = 2.4312%
Layer #130: 228 / 9376 = 2.4317%
Layer #131: 228 / 9376 = 2.4317%
Layer #132: 228 / 9376 = 2.4317%
Layer #133: 228 / 9377 = 2.4315%
Layer #134: 228 / 9375 = 2.4320%
Layer #135: 226 / 9375 = 2.4107%
Layer #136: 228 / 9378 = 2.4312%
Layer #137: 226 / 9378 = 2.4099%
Layer #138: 228 / 9376 = 2.4317%
Layer #139: 228 / 9375 = 2.4320%
Layer #140: 228 / 9375 = 2.4320%
Layer #141: 291 / 9375 = 3.1040%
