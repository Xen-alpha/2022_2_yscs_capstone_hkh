============================ PYTORCHFI INIT SUMMARY ==============================

Layer types allowing injections:
----------------------------------------------------------------------------------
   - all

Model Info:
----------------------------------------------------------------------------------
   - Shape of input into the model: (3 32 32 )
   - Batch Size: 256
   - CUDA Enabled: False

Layer Info:
----------------------------------------------------------------------------------
Layer #       Layer type  Dimensions         Weight Shape         Output Shape
----------------------------------------------------------------------------------
    0         Identity           4       ['No weights']       [1, 3, 32, 32]
    1           Conv2d           4        [64, 3, 3, 3]      [1, 64, 32, 32]
    2      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    3             ReLU           4       ['No weights']      [1, 64, 32, 32]
    4        MaxPool2d           4       ['No weights']      [1, 64, 16, 16]
    5           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
    6      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
    7             ReLU           4       ['No weights']      [1, 64, 16, 16]
    8           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
    9      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   10             ReLU           4       ['No weights']      [1, 64, 16, 16]
   11           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
   12      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   13             ReLU           4       ['No weights']      [1, 64, 16, 16]
   14           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
   15      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   16             ReLU           4       ['No weights']      [1, 64, 16, 16]
   17           Conv2d           4      [128, 64, 3, 3]       [1, 128, 8, 8]
   18      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   19             ReLU           4       ['No weights']       [1, 128, 8, 8]
   20           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   21      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   22           Conv2d           4      [128, 64, 1, 1]       [1, 128, 8, 8]
   23      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   24             ReLU           4       ['No weights']       [1, 128, 8, 8]
   25           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   26      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   27             ReLU           4       ['No weights']       [1, 128, 8, 8]
   28           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   29      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   30             ReLU           4       ['No weights']       [1, 128, 8, 8]
   31           Conv2d           4     [256, 128, 3, 3]       [1, 256, 4, 4]
   32      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   33             ReLU           4       ['No weights']       [1, 256, 4, 4]
   34           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   35      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   36           Conv2d           4     [256, 128, 1, 1]       [1, 256, 4, 4]
   37      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   38             ReLU           4       ['No weights']       [1, 256, 4, 4]
   39           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   40      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   41             ReLU           4       ['No weights']       [1, 256, 4, 4]
   42           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   43      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   44             ReLU           4       ['No weights']       [1, 256, 4, 4]
   45           Conv2d           4     [512, 256, 3, 3]       [1, 512, 2, 2]
   46      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   47             ReLU           4       ['No weights']       [1, 512, 2, 2]
   48           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   49      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   50           Conv2d           4     [512, 256, 1, 1]       [1, 512, 2, 2]
   51      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   52             ReLU           4       ['No weights']       [1, 512, 2, 2]
   53           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   54      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   55             ReLU           4       ['No weights']       [1, 512, 2, 2]
   56           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   57      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   58             ReLU           4       ['No weights']       [1, 512, 2, 2]
   59  AdaptiveAvgPool2d           4       ['No weights']       [1, 512, 1, 1]
   60           Linear           2            [10, 512]              [1, 10]
==================================================================================


===== Result =====
Seed: 12345678
Layer #1: 8 / 237 = 3.3755%
Layer #2: 8 / 237 = 3.3755%
Layer #5: 8 / 237 = 3.3755%
Layer #6: 8 / 237 = 3.3755%
Layer #8: 8 / 237 = 3.3755%
Layer #9: 7 / 237 = 2.9536%
Layer #11: 8 / 237 = 3.3755%
Layer #12: 8 / 237 = 3.3755%
Layer #14: 8 / 237 = 3.3755%
Layer #15: 8 / 237 = 3.3755%
Layer #17: 8 / 237 = 3.3755%
Layer #18: 8 / 237 = 3.3755%
Layer #20: 8 / 237 = 3.3755%
Layer #21: 8 / 237 = 3.3755%
Layer #22: 8 / 237 = 3.3755%
Layer #23: 8 / 237 = 3.3755%
Layer #25: 8 / 237 = 3.3755%
Layer #26: 8 / 237 = 3.3755%
Layer #28: 8 / 237 = 3.3755%
Layer #29: 8 / 237 = 3.3755%
Layer #31: 8 / 237 = 3.3755%
Layer #32: 22 / 237 = 9.2827%
Layer #34: 8 / 237 = 3.3755%
Layer #35: 8 / 237 = 3.3755%
Layer #36: 8 / 237 = 3.3755%
Layer #37: 8 / 237 = 3.3755%
Layer #39: 8 / 237 = 3.3755%
Layer #40: 8 / 237 = 3.3755%
Layer #42: 8 / 237 = 3.3755%
Layer #43: 8 / 237 = 3.3755%
Layer #45: 8 / 237 = 3.3755%
Layer #46: 8 / 237 = 3.3755%
Layer #48: 8 / 237 = 3.3755%
Layer #49: 8 / 237 = 3.3755%
Layer #50: 8 / 237 = 3.3755%
Layer #51: 8 / 237 = 3.3755%
Layer #53: 8 / 237 = 3.3755%
Layer #54: 8 / 237 = 3.3755%
Layer #56: 8 / 237 = 3.3755%
Layer #57: 8 / 237 = 3.3755%
Layer #60: 8 / 237 = 3.3755%
