============================ PYTORCHFI INIT SUMMARY ==============================

Layer types allowing injections:
----------------------------------------------------------------------------------
   - all

Model Info:
----------------------------------------------------------------------------------
   - Shape of input into the model: (3 32 32 )
   - Batch Size: 256
   - CUDA Enabled: True

Layer Info:
----------------------------------------------------------------------------------
Layer #       Layer type  Dimensions         Weight Shape         Output Shape
----------------------------------------------------------------------------------
    0         Identity           4       ['No weights']       [1, 3, 32, 32]
    1           Conv2d           4        [64, 3, 3, 3]      [1, 64, 32, 32]
    2      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    3             ReLU           4       ['No weights']      [1, 64, 32, 32]
    4        MaxPool2d           4       ['No weights']      [1, 64, 16, 16]
    5           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
    6      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
    7             ReLU           4       ['No weights']      [1, 64, 16, 16]
    8           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
    9      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   10             ReLU           4       ['No weights']      [1, 64, 16, 16]
   11           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
   12      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   13             ReLU           4       ['No weights']      [1, 64, 16, 16]
   14           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
   15      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   16             ReLU           4       ['No weights']      [1, 64, 16, 16]
   17           Conv2d           4      [128, 64, 3, 3]       [1, 128, 8, 8]
   18      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   19             ReLU           4       ['No weights']       [1, 128, 8, 8]
   20           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   21      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   22           Conv2d           4      [128, 64, 1, 1]       [1, 128, 8, 8]
   23      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   24             ReLU           4       ['No weights']       [1, 128, 8, 8]
   25           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   26      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   27             ReLU           4       ['No weights']       [1, 128, 8, 8]
   28           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   29      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   30             ReLU           4       ['No weights']       [1, 128, 8, 8]
   31           Conv2d           4     [256, 128, 3, 3]       [1, 256, 4, 4]
   32      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   33             ReLU           4       ['No weights']       [1, 256, 4, 4]
   34           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   35      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   36           Conv2d           4     [256, 128, 1, 1]       [1, 256, 4, 4]
   37      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   38             ReLU           4       ['No weights']       [1, 256, 4, 4]
   39           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   40      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   41             ReLU           4       ['No weights']       [1, 256, 4, 4]
   42           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   43      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   44             ReLU           4       ['No weights']       [1, 256, 4, 4]
   45           Conv2d           4     [512, 256, 3, 3]       [1, 512, 2, 2]
   46      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   47             ReLU           4       ['No weights']       [1, 512, 2, 2]
   48           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   49      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   50           Conv2d           4     [512, 256, 1, 1]       [1, 512, 2, 2]
   51      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   52             ReLU           4       ['No weights']       [1, 512, 2, 2]
   53           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   54      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   55             ReLU           4       ['No weights']       [1, 512, 2, 2]
   56           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   57      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   58             ReLU           4       ['No weights']       [1, 512, 2, 2]
   59  AdaptiveAvgPool2d           4       ['No weights']       [1, 512, 1, 1]
   60           Linear           2            [10, 512]              [1, 10]
==================================================================================


===== Result =====
Quantization bits: 32
Layer #0: 287 / 9291 = 3.0890%
Layer #1: 279 / 9291 = 3.0029%
Layer #2: 284 / 9293 = 3.0561%
Layer #3: 286 / 9291 = 3.0782%
Layer #4: 284 / 9293 = 3.0561%
Layer #5: 285 / 9292 = 3.0672%
Layer #6: 286 / 9292 = 3.0779%
Layer #7: 285 / 9292 = 3.0672%
Layer #8: 285 / 9292 = 3.0672%
Layer #9: 282 / 9291 = 3.0352%
Layer #10: 284 / 9292 = 3.0564%
Layer #11: 284 / 9292 = 3.0564%
Layer #12: 284 / 9292 = 3.0564%
Layer #13: 286 / 9292 = 3.0779%
Layer #14: 282 / 9294 = 3.0342%
Layer #15: 285 / 9292 = 3.0672%
Layer #16: 284 / 9293 = 3.0561%
Layer #17: 284 / 9292 = 3.0564%
Layer #18: 283 / 9293 = 3.0453%
Layer #19: 283 / 9291 = 3.0460%
Layer #20: 282 / 9291 = 3.0352%
Layer #21: 281 / 9291 = 3.0244%
Layer #22: 284 / 9291 = 3.0567%
Layer #23: 283 / 9291 = 3.0460%
Layer #24: 285 / 9293 = 3.0668%
Layer #25: 283 / 9291 = 3.0460%
Layer #26: 287 / 9292 = 3.0887%
Layer #27: 288 / 9293 = 3.0991%
Layer #28: 283 / 9292 = 3.0456%
Layer #29: 286 / 9292 = 3.0779%
Layer #30: 282 / 9291 = 3.0352%
Layer #31: 283 / 9292 = 3.0456%
Layer #32: 286 / 9291 = 3.0782%
Layer #33: 281 / 9292 = 3.0241%
Layer #34: 284 / 9291 = 3.0567%
Layer #35: 287 / 9293 = 3.0883%
Layer #36: 284 / 9291 = 3.0567%
Layer #37: 286 / 9292 = 3.0779%
Layer #38: 283 / 9291 = 3.0460%
Layer #39: 285 / 9292 = 3.0672%
Layer #40: 284 / 9292 = 3.0564%
Layer #41: 284 / 9292 = 3.0564%
Layer #42: 283 / 9293 = 3.0453%
Layer #43: 280 / 9292 = 3.0133%
Layer #44: 283 / 9291 = 3.0460%
Layer #45: 284 / 9291 = 3.0567%
Layer #46: 283 / 9293 = 3.0453%
Layer #47: 284 / 9294 = 3.0557%
Layer #48: 284 / 9294 = 3.0557%
Layer #49: 284 / 9293 = 3.0561%
Layer #50: 282 / 9292 = 3.0349%
Layer #51: 283 / 9291 = 3.0460%
Layer #52: 284 / 9292 = 3.0564%
Layer #53: 284 / 9292 = 3.0564%
Layer #54: 283 / 9292 = 3.0456%
Layer #55: 284 / 9291 = 3.0567%
Layer #56: 284 / 9292 = 3.0564%
Layer #57: 281 / 9293 = 3.0238%
Layer #58: 284 / 9291 = 3.0567%
Layer #59: 284 / 9291 = 3.0567%
Layer #60: 524 / 9291 = 5.6399%
