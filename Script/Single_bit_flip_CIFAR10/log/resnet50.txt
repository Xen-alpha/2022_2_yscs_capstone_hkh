============================ PYTORCHFI INIT SUMMARY ==============================

Layer types allowing injections:
----------------------------------------------------------------------------------
   - all

Model Info:
----------------------------------------------------------------------------------
   - Shape of input into the model: (3 32 32 )
   - Batch Size: 256
   - CUDA Enabled: True

Layer Info:
----------------------------------------------------------------------------------
Layer #       Layer type  Dimensions         Weight Shape         Output Shape
----------------------------------------------------------------------------------
    0         Identity           4       ['No weights']       [1, 3, 32, 32]
    1           Conv2d           4        [64, 3, 3, 3]      [1, 64, 32, 32]
    2      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    3             ReLU           4       ['No weights']      [1, 64, 32, 32]
    4        MaxPool2d           4       ['No weights']      [1, 64, 16, 16]
    5           Conv2d           4       [64, 64, 1, 1]      [1, 64, 16, 16]
    6      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
    7             ReLU           4       ['No weights']      [1, 64, 16, 16]
    8           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
    9      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   10             ReLU           4       ['No weights']      [1, 64, 16, 16]
   11           Conv2d           4      [256, 64, 1, 1]     [1, 256, 16, 16]
   12      BatchNorm2d           4                [256]     [1, 256, 16, 16]
   13           Conv2d           4      [256, 64, 1, 1]     [1, 256, 16, 16]
   14      BatchNorm2d           4                [256]     [1, 256, 16, 16]
   15             ReLU           4       ['No weights']     [1, 256, 16, 16]
   16           Conv2d           4      [64, 256, 1, 1]      [1, 64, 16, 16]
   17      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   18             ReLU           4       ['No weights']      [1, 64, 16, 16]
   19           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
   20      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   21             ReLU           4       ['No weights']      [1, 64, 16, 16]
   22           Conv2d           4      [256, 64, 1, 1]     [1, 256, 16, 16]
   23      BatchNorm2d           4                [256]     [1, 256, 16, 16]
   24             ReLU           4       ['No weights']     [1, 256, 16, 16]
   25           Conv2d           4      [64, 256, 1, 1]      [1, 64, 16, 16]
   26      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   27             ReLU           4       ['No weights']      [1, 64, 16, 16]
   28           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
   29      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   30             ReLU           4       ['No weights']      [1, 64, 16, 16]
   31           Conv2d           4      [256, 64, 1, 1]     [1, 256, 16, 16]
   32      BatchNorm2d           4                [256]     [1, 256, 16, 16]
   33             ReLU           4       ['No weights']     [1, 256, 16, 16]
   34           Conv2d           4     [128, 256, 1, 1]     [1, 128, 16, 16]
   35      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   36             ReLU           4       ['No weights']     [1, 128, 16, 16]
   37           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   38      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   39             ReLU           4       ['No weights']       [1, 128, 8, 8]
   40           Conv2d           4     [512, 128, 1, 1]       [1, 512, 8, 8]
   41      BatchNorm2d           4                [512]       [1, 512, 8, 8]
   42           Conv2d           4     [512, 256, 1, 1]       [1, 512, 8, 8]
   43      BatchNorm2d           4                [512]       [1, 512, 8, 8]
   44             ReLU           4       ['No weights']       [1, 512, 8, 8]
   45           Conv2d           4     [128, 512, 1, 1]       [1, 128, 8, 8]
   46      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   47             ReLU           4       ['No weights']       [1, 128, 8, 8]
   48           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   49      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   50             ReLU           4       ['No weights']       [1, 128, 8, 8]
   51           Conv2d           4     [512, 128, 1, 1]       [1, 512, 8, 8]
   52      BatchNorm2d           4                [512]       [1, 512, 8, 8]
   53             ReLU           4       ['No weights']       [1, 512, 8, 8]
   54           Conv2d           4     [128, 512, 1, 1]       [1, 128, 8, 8]
   55      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   56             ReLU           4       ['No weights']       [1, 128, 8, 8]
   57           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   58      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   59             ReLU           4       ['No weights']       [1, 128, 8, 8]
   60           Conv2d           4     [512, 128, 1, 1]       [1, 512, 8, 8]
   61      BatchNorm2d           4                [512]       [1, 512, 8, 8]
   62             ReLU           4       ['No weights']       [1, 512, 8, 8]
   63           Conv2d           4     [128, 512, 1, 1]       [1, 128, 8, 8]
   64      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   65             ReLU           4       ['No weights']       [1, 128, 8, 8]
   66           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   67      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   68             ReLU           4       ['No weights']       [1, 128, 8, 8]
   69           Conv2d           4     [512, 128, 1, 1]       [1, 512, 8, 8]
   70      BatchNorm2d           4                [512]       [1, 512, 8, 8]
   71             ReLU           4       ['No weights']       [1, 512, 8, 8]
   72           Conv2d           4     [256, 512, 1, 1]       [1, 256, 8, 8]
   73      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   74             ReLU           4       ['No weights']       [1, 256, 8, 8]
   75           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   76      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   77             ReLU           4       ['No weights']       [1, 256, 4, 4]
   78           Conv2d           4    [1024, 256, 1, 1]      [1, 1024, 4, 4]
   79      BatchNorm2d           4               [1024]      [1, 1024, 4, 4]
   80           Conv2d           4    [1024, 512, 1, 1]      [1, 1024, 4, 4]
   81      BatchNorm2d           4               [1024]      [1, 1024, 4, 4]
   82             ReLU           4       ['No weights']      [1, 1024, 4, 4]
   83           Conv2d           4    [256, 1024, 1, 1]       [1, 256, 4, 4]
   84      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   85             ReLU           4       ['No weights']       [1, 256, 4, 4]
   86           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   87      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   88             ReLU           4       ['No weights']       [1, 256, 4, 4]
   89           Conv2d           4    [1024, 256, 1, 1]      [1, 1024, 4, 4]
   90      BatchNorm2d           4               [1024]      [1, 1024, 4, 4]
   91             ReLU           4       ['No weights']      [1, 1024, 4, 4]
   92           Conv2d           4    [256, 1024, 1, 1]       [1, 256, 4, 4]
   93      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   94             ReLU           4       ['No weights']       [1, 256, 4, 4]
   95           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   96      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   97             ReLU           4       ['No weights']       [1, 256, 4, 4]
   98           Conv2d           4    [1024, 256, 1, 1]      [1, 1024, 4, 4]
   99      BatchNorm2d           4               [1024]      [1, 1024, 4, 4]
  100             ReLU           4       ['No weights']      [1, 1024, 4, 4]
  101           Conv2d           4    [256, 1024, 1, 1]       [1, 256, 4, 4]
  102      BatchNorm2d           4                [256]       [1, 256, 4, 4]
  103             ReLU           4       ['No weights']       [1, 256, 4, 4]
  104           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
  105      BatchNorm2d           4                [256]       [1, 256, 4, 4]
  106             ReLU           4       ['No weights']       [1, 256, 4, 4]
  107           Conv2d           4    [1024, 256, 1, 1]      [1, 1024, 4, 4]
  108      BatchNorm2d           4               [1024]      [1, 1024, 4, 4]
  109             ReLU           4       ['No weights']      [1, 1024, 4, 4]
  110           Conv2d           4    [256, 1024, 1, 1]       [1, 256, 4, 4]
  111      BatchNorm2d           4                [256]       [1, 256, 4, 4]
  112             ReLU           4       ['No weights']       [1, 256, 4, 4]
  113           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
  114      BatchNorm2d           4                [256]       [1, 256, 4, 4]
  115             ReLU           4       ['No weights']       [1, 256, 4, 4]
  116           Conv2d           4    [1024, 256, 1, 1]      [1, 1024, 4, 4]
  117      BatchNorm2d           4               [1024]      [1, 1024, 4, 4]
  118             ReLU           4       ['No weights']      [1, 1024, 4, 4]
  119           Conv2d           4    [256, 1024, 1, 1]       [1, 256, 4, 4]
  120      BatchNorm2d           4                [256]       [1, 256, 4, 4]
  121             ReLU           4       ['No weights']       [1, 256, 4, 4]
  122           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
  123      BatchNorm2d           4                [256]       [1, 256, 4, 4]
  124             ReLU           4       ['No weights']       [1, 256, 4, 4]
  125           Conv2d           4    [1024, 256, 1, 1]      [1, 1024, 4, 4]
  126      BatchNorm2d           4               [1024]      [1, 1024, 4, 4]
  127             ReLU           4       ['No weights']      [1, 1024, 4, 4]
  128           Conv2d           4    [512, 1024, 1, 1]       [1, 512, 4, 4]
  129      BatchNorm2d           4                [512]       [1, 512, 4, 4]
  130             ReLU           4       ['No weights']       [1, 512, 4, 4]
  131           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
  132      BatchNorm2d           4                [512]       [1, 512, 2, 2]
  133             ReLU           4       ['No weights']       [1, 512, 2, 2]
  134           Conv2d           4    [2048, 512, 1, 1]      [1, 2048, 2, 2]
  135      BatchNorm2d           4               [2048]      [1, 2048, 2, 2]
  136           Conv2d           4   [2048, 1024, 1, 1]      [1, 2048, 2, 2]
  137      BatchNorm2d           4               [2048]      [1, 2048, 2, 2]
  138             ReLU           4       ['No weights']      [1, 2048, 2, 2]
  139           Conv2d           4    [512, 2048, 1, 1]       [1, 512, 2, 2]
  140      BatchNorm2d           4                [512]       [1, 512, 2, 2]
  141             ReLU           4       ['No weights']       [1, 512, 2, 2]
  142           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
  143      BatchNorm2d           4                [512]       [1, 512, 2, 2]
  144             ReLU           4       ['No weights']       [1, 512, 2, 2]
  145           Conv2d           4    [2048, 512, 1, 1]      [1, 2048, 2, 2]
  146      BatchNorm2d           4               [2048]      [1, 2048, 2, 2]
  147             ReLU           4       ['No weights']      [1, 2048, 2, 2]
  148           Conv2d           4    [512, 2048, 1, 1]       [1, 512, 2, 2]
  149      BatchNorm2d           4                [512]       [1, 512, 2, 2]
  150             ReLU           4       ['No weights']       [1, 512, 2, 2]
  151           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
  152      BatchNorm2d           4                [512]       [1, 512, 2, 2]
  153             ReLU           4       ['No weights']       [1, 512, 2, 2]
  154           Conv2d           4    [2048, 512, 1, 1]      [1, 2048, 2, 2]
  155      BatchNorm2d           4               [2048]      [1, 2048, 2, 2]
  156             ReLU           4       ['No weights']      [1, 2048, 2, 2]
  157  AdaptiveAvgPool2d           4       ['No weights']      [1, 2048, 1, 1]
  158           Linear           2           [10, 2048]              [1, 10]
==================================================================================


===== Result =====
Quantization bits: 32
Layer #0: 304 / 9349 = 3.2517%
Layer #1: 302 / 9349 = 3.2303%
Layer #2: 300 / 9350 = 3.2086%
Layer #3: 306 / 9351 = 3.2724%
Layer #4: 305 / 9351 = 3.2617%
Layer #5: 301 / 9350 = 3.2193%
Layer #6: 303 / 9350 = 3.2406%
Layer #7: 302 / 9349 = 3.2303%
Layer #8: 303 / 9349 = 3.2410%
Layer #9: 301 / 9351 = 3.2189%
Layer #10: 303 / 9350 = 3.2406%
Layer #11: 301 / 9349 = 3.2196%
Layer #12: 302 / 9349 = 3.2303%
Layer #13: 301 / 9350 = 3.2193%
Layer #14: 302 / 9352 = 3.2293%
Layer #15: 301 / 9350 = 3.2193%
Layer #16: 302 / 9351 = 3.2296%
Layer #17: 302 / 9350 = 3.2299%
Layer #18: 302 / 9350 = 3.2299%
Layer #19: 301 / 9349 = 3.2196%
Layer #20: 302 / 9350 = 3.2299%
Layer #21: 301 / 9353 = 3.2182%
Layer #22: 301 / 9349 = 3.2196%
Layer #23: 302 / 9350 = 3.2299%
Layer #24: 302 / 9349 = 3.2303%
Layer #25: 303 / 9351 = 3.2403%
Layer #26: 302 / 9349 = 3.2303%
Layer #27: 301 / 9350 = 3.2193%
Layer #28: 300 / 9351 = 3.2082%
Layer #29: 302 / 9350 = 3.2299%
Layer #30: 299 / 9350 = 3.1979%
Layer #31: 302 / 9349 = 3.2303%
Layer #32: 301 / 9350 = 3.2193%
Layer #33: 300 / 9349 = 3.2089%
Layer #34: 300 / 9349 = 3.2089%
Layer #35: 301 / 9351 = 3.2189%
Layer #36: 300 / 9350 = 3.2086%
Layer #37: 303 / 9350 = 3.2406%
Layer #38: 303 / 9349 = 3.2410%
Layer #39: 301 / 9350 = 3.2193%
Layer #40: 302 / 9350 = 3.2299%
Layer #41: 302 / 9349 = 3.2303%
Layer #42: 301 / 9352 = 3.2186%
Layer #43: 299 / 9351 = 3.1975%
Layer #44: 303 / 9349 = 3.2410%
Layer #45: 301 / 9350 = 3.2193%
Layer #46: 304 / 9351 = 3.2510%
Layer #47: 305 / 9351 = 3.2617%
Layer #48: 301 / 9350 = 3.2193%
Layer #49: 300 / 9350 = 3.2086%
Layer #50: 301 / 9350 = 3.2193%
Layer #51: 302 / 9349 = 3.2303%
Layer #52: 301 / 9349 = 3.2196%
Layer #53: 300 / 9352 = 3.2079%
Layer #54: 302 / 9350 = 3.2299%
Layer #55: 303 / 9349 = 3.2410%
Layer #56: 301 / 9349 = 3.2196%
Layer #57: 303 / 9352 = 3.2399%
Layer #58: 301 / 9349 = 3.2196%
Layer #59: 302 / 9350 = 3.2299%
Layer #60: 301 / 9350 = 3.2193%
Layer #61: 301 / 9349 = 3.2196%
Layer #62: 301 / 9353 = 3.2182%
Layer #63: 302 / 9349 = 3.2303%
Layer #64: 302 / 9350 = 3.2299%
Layer #65: 305 / 9350 = 3.2620%
Layer #66: 301 / 9349 = 3.2196%
Layer #67: 300 / 9351 = 3.2082%
Layer #68: 301 / 9351 = 3.2189%
Layer #69: 301 / 9349 = 3.2196%
Layer #70: 302 / 9349 = 3.2303%
Layer #71: 301 / 9350 = 3.2193%
Layer #72: 301 / 9350 = 3.2193%
Layer #73: 305 / 9349 = 3.2624%
Layer #74: 303 / 9352 = 3.2399%
Layer #75: 300 / 9351 = 3.2082%
Layer #76: 303 / 9350 = 3.2406%
Layer #77: 303 / 9349 = 3.2410%
Layer #78: 302 / 9351 = 3.2296%
Layer #79: 301 / 9352 = 3.2186%
Layer #80: 301 / 9350 = 3.2193%
Layer #81: 302 / 9353 = 3.2289%
Layer #82: 300 / 9351 = 3.2082%
Layer #83: 300 / 9349 = 3.2089%
Layer #84: 302 / 9351 = 3.2296%
Layer #85: 301 / 9351 = 3.2189%
Layer #86: 301 / 9349 = 3.2196%
Layer #87: 300 / 9349 = 3.2089%
Layer #88: 299 / 9350 = 3.1979%
Layer #89: 300 / 9349 = 3.2089%
Layer #90: 302 / 9350 = 3.2299%
Layer #91: 302 / 9350 = 3.2299%
Layer #92: 300 / 9349 = 3.2089%
Layer #93: 302 / 9351 = 3.2296%
Layer #94: 299 / 9350 = 3.1979%
Layer #95: 300 / 9350 = 3.2086%
Layer #96: 302 / 9350 = 3.2299%
Layer #97: 304 / 9349 = 3.2517%
Layer #98: 302 / 9351 = 3.2296%
Layer #99: 301 / 9349 = 3.2196%
Layer #100: 302 / 9351 = 3.2296%
Layer #101: 300 / 9352 = 3.2079%
Layer #102: 301 / 9350 = 3.2193%
Layer #103: 300 / 9349 = 3.2089%
Layer #104: 300 / 9349 = 3.2089%
Layer #105: 301 / 9350 = 3.2193%
Layer #106: 300 / 9349 = 3.2089%
Layer #107: 301 / 9350 = 3.2193%
Layer #108: 302 / 9349 = 3.2303%
Layer #109: 303 / 9350 = 3.2406%
Layer #110: 302 / 9349 = 3.2303%
Layer #111: 303 / 9350 = 3.2406%
Layer #112: 301 / 9349 = 3.2196%
Layer #113: 303 / 9351 = 3.2403%
Layer #114: 302 / 9350 = 3.2299%
Layer #115: 299 / 9350 = 3.1979%
Layer #116: 302 / 9350 = 3.2299%
Layer #117: 302 / 9350 = 3.2299%
Layer #118: 302 / 9353 = 3.2289%
Layer #119: 302 / 9349 = 3.2303%
Layer #120: 301 / 9350 = 3.2193%
Layer #121: 301 / 9349 = 3.2196%
Layer #122: 302 / 9350 = 3.2299%
Layer #123: 301 / 9349 = 3.2196%
Layer #124: 302 / 9353 = 3.2289%
Layer #125: 302 / 9349 = 3.2303%
Layer #126: 302 / 9349 = 3.2303%
Layer #127: 299 / 9351 = 3.1975%
Layer #128: 302 / 9350 = 3.2299%
Layer #129: 301 / 9350 = 3.2193%
Layer #130: 300 / 9350 = 3.2086%
Layer #131: 302 / 9350 = 3.2299%
Layer #132: 301 / 9351 = 3.2189%
Layer #133: 300 / 9351 = 3.2082%
Layer #134: 302 / 9349 = 3.2303%
Layer #135: 302 / 9349 = 3.2303%
Layer #136: 302 / 9352 = 3.2293%
Layer #137: 300 / 9351 = 3.2082%
Layer #138: 302 / 9349 = 3.2303%
Layer #139: 302 / 9349 = 3.2303%
Layer #140: 299 / 9349 = 3.1982%
Layer #141: 302 / 9350 = 3.2299%
Layer #142: 302 / 9350 = 3.2299%
Layer #143: 302 / 9350 = 3.2299%
Layer #144: 302 / 9350 = 3.2299%
Layer #145: 300 / 9350 = 3.2086%
Layer #146: 302 / 9351 = 3.2296%
Layer #147: 301 / 9351 = 3.2189%
Layer #148: 302 / 9351 = 3.2296%
Layer #149: 302 / 9350 = 3.2299%
Layer #150: 302 / 9350 = 3.2299%
Layer #151: 301 / 9350 = 3.2193%
Layer #152: 302 / 9351 = 3.2296%
Layer #153: 300 / 9349 = 3.2089%
Layer #154: 302 / 9349 = 3.2303%
Layer #155: 299 / 9350 = 3.1979%
Layer #156: 301 / 9350 = 3.2193%
Layer #157: 302 / 9349 = 3.2303%
Layer #158: 413 / 9350 = 4.4171%
