============================ PYTORCHFI INIT SUMMARY ==============================

Layer types allowing injections:
----------------------------------------------------------------------------------
   - all

Model Info:
----------------------------------------------------------------------------------
   - Shape of input into the model: (3 32 32 )
   - Batch Size: 256
   - CUDA Enabled: True

Layer Info:
----------------------------------------------------------------------------------
Layer #       Layer type  Dimensions         Weight Shape         Output Shape
----------------------------------------------------------------------------------
    0         Identity           4       ['No weights']       [1, 3, 32, 32]
    1           Conv2d           4        [64, 3, 3, 3]      [1, 64, 32, 32]
    2      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    3             ReLU           4       ['No weights']      [1, 64, 32, 32]
    4        MaxPool2d           4       ['No weights']      [1, 64, 16, 16]
    5           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
    6      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
    7             ReLU           4       ['No weights']      [1, 64, 16, 16]
    8           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
    9      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   10             ReLU           4       ['No weights']      [1, 64, 16, 16]
   11           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
   12      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   13             ReLU           4       ['No weights']      [1, 64, 16, 16]
   14           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
   15      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   16             ReLU           4       ['No weights']      [1, 64, 16, 16]
   17           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
   18      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   19             ReLU           4       ['No weights']      [1, 64, 16, 16]
   20           Conv2d           4       [64, 64, 3, 3]      [1, 64, 16, 16]
   21      BatchNorm2d           4                 [64]      [1, 64, 16, 16]
   22             ReLU           4       ['No weights']      [1, 64, 16, 16]
   23           Conv2d           4      [128, 64, 3, 3]       [1, 128, 8, 8]
   24      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   25             ReLU           4       ['No weights']       [1, 128, 8, 8]
   26           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   27      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   28           Conv2d           4      [128, 64, 1, 1]       [1, 128, 8, 8]
   29      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   30             ReLU           4       ['No weights']       [1, 128, 8, 8]
   31           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   32      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   33             ReLU           4       ['No weights']       [1, 128, 8, 8]
   34           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   35      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   36             ReLU           4       ['No weights']       [1, 128, 8, 8]
   37           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   38      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   39             ReLU           4       ['No weights']       [1, 128, 8, 8]
   40           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   41      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   42             ReLU           4       ['No weights']       [1, 128, 8, 8]
   43           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   44      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   45             ReLU           4       ['No weights']       [1, 128, 8, 8]
   46           Conv2d           4     [128, 128, 3, 3]       [1, 128, 8, 8]
   47      BatchNorm2d           4                [128]       [1, 128, 8, 8]
   48             ReLU           4       ['No weights']       [1, 128, 8, 8]
   49           Conv2d           4     [256, 128, 3, 3]       [1, 256, 4, 4]
   50      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   51             ReLU           4       ['No weights']       [1, 256, 4, 4]
   52           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   53      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   54           Conv2d           4     [256, 128, 1, 1]       [1, 256, 4, 4]
   55      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   56             ReLU           4       ['No weights']       [1, 256, 4, 4]
   57           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   58      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   59             ReLU           4       ['No weights']       [1, 256, 4, 4]
   60           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   61      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   62             ReLU           4       ['No weights']       [1, 256, 4, 4]
   63           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   64      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   65             ReLU           4       ['No weights']       [1, 256, 4, 4]
   66           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   67      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   68             ReLU           4       ['No weights']       [1, 256, 4, 4]
   69           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   70      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   71             ReLU           4       ['No weights']       [1, 256, 4, 4]
   72           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   73      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   74             ReLU           4       ['No weights']       [1, 256, 4, 4]
   75           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   76      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   77             ReLU           4       ['No weights']       [1, 256, 4, 4]
   78           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   79      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   80             ReLU           4       ['No weights']       [1, 256, 4, 4]
   81           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   82      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   83             ReLU           4       ['No weights']       [1, 256, 4, 4]
   84           Conv2d           4     [256, 256, 3, 3]       [1, 256, 4, 4]
   85      BatchNorm2d           4                [256]       [1, 256, 4, 4]
   86             ReLU           4       ['No weights']       [1, 256, 4, 4]
   87           Conv2d           4     [512, 256, 3, 3]       [1, 512, 2, 2]
   88      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   89             ReLU           4       ['No weights']       [1, 512, 2, 2]
   90           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   91      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   92           Conv2d           4     [512, 256, 1, 1]       [1, 512, 2, 2]
   93      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   94             ReLU           4       ['No weights']       [1, 512, 2, 2]
   95           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   96      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   97             ReLU           4       ['No weights']       [1, 512, 2, 2]
   98           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   99      BatchNorm2d           4                [512]       [1, 512, 2, 2]
  100             ReLU           4       ['No weights']       [1, 512, 2, 2]
  101           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
  102      BatchNorm2d           4                [512]       [1, 512, 2, 2]
  103             ReLU           4       ['No weights']       [1, 512, 2, 2]
  104           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
  105      BatchNorm2d           4                [512]       [1, 512, 2, 2]
  106             ReLU           4       ['No weights']       [1, 512, 2, 2]
  107  AdaptiveAvgPool2d           4       ['No weights']       [1, 512, 1, 1]
  108           Linear           2            [10, 512]              [1, 10]
==================================================================================


===== Result =====
Quantization bits: 32
Layer #0: 273 / 9317 = 2.9301%
Layer #1: 272 / 9317 = 2.9194%
Layer #2: 270 / 9318 = 2.8976%
Layer #3: 271 / 9318 = 2.9083%
Layer #4: 272 / 9319 = 2.9188%
Layer #5: 272 / 9318 = 2.9191%
Layer #6: 272 / 9318 = 2.9191%
Layer #7: 271 / 9317 = 2.9087%
Layer #8: 270 / 9318 = 2.8976%
Layer #9: 270 / 9317 = 2.8979%
Layer #10: 271 / 9318 = 2.9083%
Layer #11: 273 / 9318 = 2.9298%
Layer #12: 272 / 9319 = 2.9188%
Layer #13: 271 / 9317 = 2.9087%
Layer #14: 272 / 9320 = 2.9185%
Layer #15: 271 / 9318 = 2.9083%
Layer #16: 272 / 9318 = 2.9191%
Layer #17: 271 / 9318 = 2.9083%
Layer #18: 272 / 9317 = 2.9194%
Layer #19: 271 / 9317 = 2.9087%
Layer #20: 273 / 9317 = 2.9301%
Layer #21: 268 / 9318 = 2.8762%
Layer #22: 271 / 9318 = 2.9083%
Layer #23: 271 / 9317 = 2.9087%
Layer #24: 272 / 9318 = 2.9191%
Layer #25: 274 / 9319 = 2.9402%
Layer #26: 272 / 9318 = 2.9191%
Layer #27: 272 / 9319 = 2.9188%
Layer #28: 269 / 9318 = 2.8869%
Layer #29: 271 / 9318 = 2.9083%
Layer #30: 270 / 9318 = 2.8976%
Layer #31: 272 / 9319 = 2.9188%
Layer #32: 270 / 9317 = 2.8979%
Layer #33: 271 / 9319 = 2.9080%
Layer #34: 272 / 9317 = 2.9194%
Layer #35: 270 / 9318 = 2.8976%
Layer #36: 272 / 9318 = 2.9191%
Layer #37: 272 / 9318 = 2.9191%
Layer #38: 270 / 9317 = 2.8979%
Layer #39: 272 / 9318 = 2.9191%
Layer #40: 271 / 9318 = 2.9083%
Layer #41: 269 / 9318 = 2.8869%
Layer #42: 270 / 9320 = 2.8970%
Layer #43: 271 / 9320 = 2.9077%
Layer #44: 271 / 9319 = 2.9080%
Layer #45: 270 / 9318 = 2.8976%
Layer #46: 272 / 9320 = 2.9185%
Layer #47: 271 / 9319 = 2.9080%
Layer #48: 270 / 9317 = 2.8979%
Layer #49: 274 / 9318 = 2.9405%
Layer #50: 273 / 9318 = 2.9298%
Layer #51: 271 / 9318 = 2.9083%
Layer #52: 271 / 9317 = 2.9087%
Layer #53: 272 / 9317 = 2.9194%
Layer #54: 272 / 9318 = 2.9191%
Layer #55: 272 / 9317 = 2.9194%
Layer #56: 270 / 9318 = 2.8976%
Layer #57: 272 / 9320 = 2.9185%
Layer #58: 271 / 9317 = 2.9087%
Layer #59: 272 / 9319 = 2.9188%
Layer #60: 272 / 9317 = 2.9194%
Layer #61: 272 / 9319 = 2.9188%
Layer #62: 267 / 9319 = 2.8651%
Layer #63: 272 / 9318 = 2.9191%
Layer #64: 271 / 9319 = 2.9080%
Layer #65: 270 / 9318 = 2.8976%
Layer #66: 271 / 9318 = 2.9083%
Layer #67: 271 / 9317 = 2.9087%
Layer #68: 272 / 9318 = 2.9191%
Layer #69: 271 / 9317 = 2.9087%
Layer #70: 271 / 9317 = 2.9087%
Layer #71: 271 / 9317 = 2.9087%
Layer #72: 271 / 9318 = 2.9083%
Layer #73: 271 / 9317 = 2.9087%
Layer #74: 272 / 9319 = 2.9188%
Layer #75: 271 / 9319 = 2.9080%
Layer #76: 272 / 9318 = 2.9191%
Layer #77: 271 / 9317 = 2.9087%
Layer #78: 271 / 9318 = 2.9083%
Layer #79: 270 / 9319 = 2.8973%
Layer #80: 271 / 9318 = 2.9083%
Layer #81: 270 / 9320 = 2.8970%
Layer #82: 271 / 9319 = 2.9080%
Layer #83: 272 / 9317 = 2.9194%
Layer #84: 272 / 9319 = 2.9188%
Layer #85: 272 / 9319 = 2.9188%
Layer #86: 272 / 9318 = 2.9191%
Layer #87: 272 / 9317 = 2.9194%
Layer #88: 271 / 9318 = 2.9083%
Layer #89: 272 / 9318 = 2.9191%
Layer #90: 272 / 9318 = 2.9191%
Layer #91: 272 / 9319 = 2.9188%
Layer #92: 272 / 9319 = 2.9188%
Layer #93: 271 / 9320 = 2.9077%
Layer #94: 271 / 9319 = 2.9080%
Layer #95: 272 / 9317 = 2.9194%
Layer #96: 272 / 9319 = 2.9188%
Layer #97: 271 / 9317 = 2.9087%
Layer #98: 270 / 9318 = 2.8976%
Layer #99: 272 / 9318 = 2.9191%
Layer #100: 271 / 9319 = 2.9080%
Layer #101: 272 / 9320 = 2.9185%
Layer #102: 271 / 9318 = 2.9083%
Layer #103: 272 / 9318 = 2.9191%
Layer #104: 272 / 9318 = 2.9191%
Layer #105: 272 / 9318 = 2.9191%
Layer #106: 270 / 9317 = 2.8979%
Layer #107: 271 / 9318 = 2.9083%
Layer #108: 423 / 9319 = 4.5391%
