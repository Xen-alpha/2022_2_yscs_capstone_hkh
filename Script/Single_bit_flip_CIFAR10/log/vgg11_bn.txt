============================ PYTORCHFI INIT SUMMARY ==============================

Layer types allowing injections:
----------------------------------------------------------------------------------
   - all

Model Info:
----------------------------------------------------------------------------------
   - Shape of input into the model: (3 32 32 )
   - Batch Size: 256
   - CUDA Enabled: True

Layer Info:
----------------------------------------------------------------------------------
Layer #       Layer type  Dimensions         Weight Shape         Output Shape
----------------------------------------------------------------------------------
    0         Identity           4       ['No weights']       [1, 3, 32, 32]
    1           Conv2d           4        [64, 3, 3, 3]      [1, 64, 32, 32]
    2      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    3             ReLU           4       ['No weights']      [1, 64, 32, 32]
    4        MaxPool2d           4       ['No weights']      [1, 64, 16, 16]
    5           Conv2d           4      [128, 64, 3, 3]     [1, 128, 16, 16]
    6      BatchNorm2d           4                [128]     [1, 128, 16, 16]
    7             ReLU           4       ['No weights']     [1, 128, 16, 16]
    8        MaxPool2d           4       ['No weights']       [1, 128, 8, 8]
    9           Conv2d           4     [256, 128, 3, 3]       [1, 256, 8, 8]
   10      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   11             ReLU           4       ['No weights']       [1, 256, 8, 8]
   12           Conv2d           4     [256, 256, 3, 3]       [1, 256, 8, 8]
   13      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   14             ReLU           4       ['No weights']       [1, 256, 8, 8]
   15        MaxPool2d           4       ['No weights']       [1, 256, 4, 4]
   16           Conv2d           4     [512, 256, 3, 3]       [1, 512, 4, 4]
   17      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   18             ReLU           4       ['No weights']       [1, 512, 4, 4]
   19           Conv2d           4     [512, 512, 3, 3]       [1, 512, 4, 4]
   20      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   21             ReLU           4       ['No weights']       [1, 512, 4, 4]
   22        MaxPool2d           4       ['No weights']       [1, 512, 2, 2]
   23           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   24      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   25             ReLU           4       ['No weights']       [1, 512, 2, 2]
   26           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   27      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   28             ReLU           4       ['No weights']       [1, 512, 2, 2]
   29        MaxPool2d           4       ['No weights']       [1, 512, 1, 1]
   30  AdaptiveAvgPool2d           4       ['No weights']       [1, 512, 1, 1]
   31           Linear           2          [4096, 512]            [1, 4096]
   32             ReLU           2       ['No weights']            [1, 4096]
   33          Dropout           2       ['No weights']            [1, 4096]
   34           Linear           2         [4096, 4096]            [1, 4096]
   35             ReLU           2       ['No weights']            [1, 4096]
   36          Dropout           2       ['No weights']            [1, 4096]
   37           Linear           2           [10, 4096]              [1, 10]
==================================================================================


===== Result =====
Quantization bits: 32
Layer #0: 311 / 9223 = 3.3720%
Layer #1: 313 / 9223 = 3.3937%
Layer #2: 315 / 9226 = 3.4143%
Layer #3: 313 / 9224 = 3.3933%
Layer #4: 318 / 9225 = 3.4472%
Layer #5: 316 / 9226 = 3.4251%
Layer #6: 315 / 9224 = 3.4150%
Layer #7: 316 / 9225 = 3.4255%
Layer #8: 314 / 9224 = 3.4042%
Layer #9: 313 / 9223 = 3.3937%
Layer #10: 317 / 9226 = 3.4359%
Layer #11: 316 / 9225 = 3.4255%
Layer #12: 314 / 9223 = 3.4045%
Layer #13: 312 / 9224 = 3.3825%
Layer #14: 315 / 9226 = 3.4143%
Layer #15: 318 / 9223 = 3.4479%
Layer #16: 316 / 9225 = 3.4255%
Layer #17: 314 / 9224 = 3.4042%
Layer #18: 312 / 9224 = 3.3825%
Layer #19: 315 / 9223 = 3.4154%
Layer #20: 313 / 9224 = 3.3933%
Layer #21: 312 / 9225 = 3.3821%
Layer #22: 312 / 9225 = 3.3821%
Layer #23: 315 / 9225 = 3.4146%
Layer #24: 311 / 9223 = 3.3720%
Layer #25: 314 / 9224 = 3.4042%
Layer #26: 314 / 9224 = 3.4042%
Layer #27: 315 / 9226 = 3.4143%
Layer #28: 314 / 9226 = 3.4034%
Layer #29: 314 / 9224 = 3.4042%
Layer #30: 315 / 9225 = 3.4146%
Layer #31: 315 / 9225 = 3.4146%
Layer #32: 314 / 9223 = 3.4045%
Layer #33: 315 / 9224 = 3.4150%
Layer #34: 315 / 9223 = 3.4154%
Layer #35: 315 / 9226 = 3.4143%
Layer #36: 315 / 9223 = 3.4154%
Layer #37: 471 / 9224 = 5.1062%
