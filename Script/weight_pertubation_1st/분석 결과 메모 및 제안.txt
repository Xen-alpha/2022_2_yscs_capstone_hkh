(1) mini-batch 값을 256, 512, 1024로 두고 가중치 값의 일정한 위치의 bit에 bit-flip을 시행한 결과 일단 mini-batch 크기에 따라 추론 실패 확률이 변화하긴 한다.
그러나 mini-batch 크기 변화에 따라 일정하게 증감하지 않는다고 추정된다(랜덤 변화에 가까운 것으로 추정 - 더 많은 시행 횟수와 더 많은 mini-batch size들에 대한 테스트가 필요).
(2) 레이어 Type별 bit-flip 1회 시행의 결과 유달리 취약한 레이어 종류는 발견되지 않았다(레이어 타입/레이어 위치에 상관없이 perturbation 이전 추론 대비 error rate 증가량이 0%에 가까움).
(3) 유달리 추론 실패가 심한 레이어를 분석한 결과 삽입된 가중치 에러가 원래 값 대비 어떤 범위 이상으로 변화하면 error rate가 변하는 것으로 보인다.
32bit single precision 부동 소수점 가중치를 썼을 때 지수부(exponent)에 비해 가수부(fraction/mantissa)가 bit flip에 둔감해보인다.
(4) Neuron-based Perturbation와의 비교
* 특정 구간의 입력값을 하나의 출력에 대응시키는 DNN 레이어 연산들의 특성을 생각해보면...
* 최종적으로 분류해야 할 사물을 추론하는 마지막 레이어에서 출력값의 변화에 따른 추론 실패율 증가가 가장 컸던 것을 생각해볼 때 출력 레이어에 error injection을 시행한 케이스는 출력 데이터 내 특정한 값의 큰 변화(R/G/B, 최종 레이어의 출력)에 따른 추론 결과의 변화가 쉬웠던 것으로 추론된다.
* 반면 현재 개발된 DNN 모델의 레이어들은 대부분 선형결합 형태의 연산을 하는 레이어들이라는 걸 생각했을 때 미묘한 수준의 가중치 변화로는 특정 출력을 내는 입력값 구간의 위치를 크게 변화시키기 힘들다.
* 입력 변화에 따른 기하급수적인 변화를 일으키는 연산(초월함수 등)을 사용하는 Layer가 아니라면 가중치 Error Perturbation은 Error position의 변화보다는 Error Value의 변화량에 더 큰 영향을 받는 것으로 보인다.
(5) Error Neuron과 Error Weight 중 어느 것이 더 심각한 문제인가?
* 단순 추론 실패 결과만 보면 Error Neuron이 발생한 경우가 더 추론 실패 확률이 크다.
* 그러나 우리가 연구하는 bit flip의 경우 현실에서는 낮은 확률로 일어나는 일이다.
* 데이터 출력의 Soft Error는 일시적인 한두 번의 추론 실패를 발생시키지만 가중치의 Soft Error는 한 번 발생하면 DNN 가속기 하드웨어에 반영구적으로 저장된다.
* 따라서 가중치 소프트 에러가 출력 데이터 내 소프트 에러에 비해 딱히 덜 심각한 일이라고 보기 힘들다.

(3)에서 파생된 가중치 소프트 에러 보정 연구 제안
1. 어떤 학습된 가중치가 특정 실수 구간(혹은 실수 구간들)에 밀집하는 경향에서 착안하여 그 실수 범위(들) 대신 2^n승 개의 정수 가중치 범위를 사용하는 것이 Quantization이다.
2. 어떤 Layer가 IEEE 754 single precision 부동 소수점 형식의 실수 가중치들을 가진다고 가정하자. 이 가중치는 부호 1bit, 지수 8bit, 가수 23bit로 이루어져 있다.
3. 아이디어: 만일 특정 종류의 레이어 내의 어떤 가중치에서 소수점 이하 특정 자릿수에 변화가 생겨도 대다수의 상황에서 추가적인 추론 실패 증가량이 미미할 정도로 출력값이 변화하는 경우가 매우 적다면?
4. 지수부(exponent)와 부호 비트라면 모를까, 가수부(Fraction/Mantissa)는 하위 nbit에서는 비트가 바뀌어도 가중치 실수값의 변화가 적다.
-> 가령 가수부 하위 4bit를 제거하고 가중치를 여러개 복사하여 Error Correction Code 묶음을 만든 다음 원래 가중치와 비교하여 가중치 정정을 시도할 때 그냥 가중치 값의 모든 bit를 복사하는 것에 비해 ECC 메모리 공간의 절약 효과가 제대로 드러난다고 할 수 있을까?
5. Double precision, half precision 가중치의 경우는 또 어떠한가? 정밀도 및 표현 가능한 숫자 범위의 변화와 잘라내도 무방한 하위 가수부 bit 수 변화가 비례하는가?