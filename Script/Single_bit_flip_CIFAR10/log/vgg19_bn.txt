============================ PYTORCHFI INIT SUMMARY ==============================

Layer types allowing injections:
----------------------------------------------------------------------------------
   - all

Model Info:
----------------------------------------------------------------------------------
   - Shape of input into the model: (3 32 32 )
   - Batch Size: 256
   - CUDA Enabled: True

Layer Info:
----------------------------------------------------------------------------------
Layer #       Layer type  Dimensions         Weight Shape         Output Shape
----------------------------------------------------------------------------------
    0         Identity           4       ['No weights']       [1, 3, 32, 32]
    1           Conv2d           4        [64, 3, 3, 3]      [1, 64, 32, 32]
    2      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    3             ReLU           4       ['No weights']      [1, 64, 32, 32]
    4           Conv2d           4       [64, 64, 3, 3]      [1, 64, 32, 32]
    5      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    6             ReLU           4       ['No weights']      [1, 64, 32, 32]
    7        MaxPool2d           4       ['No weights']      [1, 64, 16, 16]
    8           Conv2d           4      [128, 64, 3, 3]     [1, 128, 16, 16]
    9      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   10             ReLU           4       ['No weights']     [1, 128, 16, 16]
   11           Conv2d           4     [128, 128, 3, 3]     [1, 128, 16, 16]
   12      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   13             ReLU           4       ['No weights']     [1, 128, 16, 16]
   14        MaxPool2d           4       ['No weights']       [1, 128, 8, 8]
   15           Conv2d           4     [256, 128, 3, 3]       [1, 256, 8, 8]
   16      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   17             ReLU           4       ['No weights']       [1, 256, 8, 8]
   18           Conv2d           4     [256, 256, 3, 3]       [1, 256, 8, 8]
   19      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   20             ReLU           4       ['No weights']       [1, 256, 8, 8]
   21           Conv2d           4     [256, 256, 3, 3]       [1, 256, 8, 8]
   22      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   23             ReLU           4       ['No weights']       [1, 256, 8, 8]
   24           Conv2d           4     [256, 256, 3, 3]       [1, 256, 8, 8]
   25      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   26             ReLU           4       ['No weights']       [1, 256, 8, 8]
   27        MaxPool2d           4       ['No weights']       [1, 256, 4, 4]
   28           Conv2d           4     [512, 256, 3, 3]       [1, 512, 4, 4]
   29      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   30             ReLU           4       ['No weights']       [1, 512, 4, 4]
   31           Conv2d           4     [512, 512, 3, 3]       [1, 512, 4, 4]
   32      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   33             ReLU           4       ['No weights']       [1, 512, 4, 4]
   34           Conv2d           4     [512, 512, 3, 3]       [1, 512, 4, 4]
   35      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   36             ReLU           4       ['No weights']       [1, 512, 4, 4]
   37           Conv2d           4     [512, 512, 3, 3]       [1, 512, 4, 4]
   38      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   39             ReLU           4       ['No weights']       [1, 512, 4, 4]
   40        MaxPool2d           4       ['No weights']       [1, 512, 2, 2]
   41           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   42      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   43             ReLU           4       ['No weights']       [1, 512, 2, 2]
   44           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   45      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   46             ReLU           4       ['No weights']       [1, 512, 2, 2]
   47           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   48      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   49             ReLU           4       ['No weights']       [1, 512, 2, 2]
   50           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   51      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   52             ReLU           4       ['No weights']       [1, 512, 2, 2]
   53        MaxPool2d           4       ['No weights']       [1, 512, 1, 1]
   54  AdaptiveAvgPool2d           4       ['No weights']       [1, 512, 1, 1]
   55           Linear           2          [4096, 512]            [1, 4096]
   56             ReLU           2       ['No weights']            [1, 4096]
   57          Dropout           2       ['No weights']            [1, 4096]
   58           Linear           2         [4096, 4096]            [1, 4096]
   59             ReLU           2       ['No weights']            [1, 4096]
   60          Dropout           2       ['No weights']            [1, 4096]
   61           Linear           2           [10, 4096]              [1, 10]
==================================================================================


===== Result =====
Quantization bits: 32
Layer #0: 291 / 9379 = 3.1027%
Layer #1: 292 / 9379 = 3.1133%
Layer #2: 290 / 9381 = 3.0914%
Layer #3: 291 / 9380 = 3.1023%
Layer #4: 293 / 9382 = 3.1230%
Layer #5: 291 / 9381 = 3.1020%
Layer #6: 291 / 9380 = 3.1023%
Layer #7: 291 / 9380 = 3.1023%
Layer #8: 291 / 9379 = 3.1027%
Layer #9: 290 / 9379 = 3.0920%
Layer #10: 291 / 9381 = 3.1020%
Layer #11: 290 / 9380 = 3.0917%
Layer #12: 288 / 9382 = 3.0697%
Layer #13: 290 / 9379 = 3.0920%
Layer #14: 293 / 9382 = 3.1230%
Layer #15: 291 / 9379 = 3.1027%
Layer #16: 290 / 9381 = 3.0914%
Layer #17: 292 / 9380 = 3.1130%
Layer #18: 291 / 9380 = 3.1023%
Layer #19: 291 / 9379 = 3.1027%
Layer #20: 291 / 9379 = 3.1027%
Layer #21: 291 / 9380 = 3.1023%
Layer #22: 290 / 9381 = 3.0914%
Layer #23: 289 / 9380 = 3.0810%
Layer #24: 294 / 9380 = 3.1343%
Layer #25: 290 / 9381 = 3.0914%
Layer #26: 292 / 9379 = 3.1133%
Layer #27: 291 / 9382 = 3.1017%
Layer #28: 289 / 9380 = 3.0810%
Layer #29: 291 / 9380 = 3.1023%
Layer #30: 286 / 9380 = 3.0490%
Layer #31: 291 / 9380 = 3.1023%
Layer #32: 291 / 9379 = 3.1027%
Layer #33: 287 / 9380 = 3.0597%
Layer #34: 291 / 9379 = 3.1027%
Layer #35: 290 / 9380 = 3.0917%
Layer #36: 290 / 9379 = 3.0920%
Layer #37: 290 / 9380 = 3.0917%
Layer #38: 289 / 9379 = 3.0814%
Layer #39: 291 / 9381 = 3.1020%
Layer #40: 290 / 9380 = 3.0917%
Layer #41: 291 / 9379 = 3.1027%
Layer #42: 290 / 9383 = 3.0907%
Layer #43: 291 / 9382 = 3.1017%
Layer #44: 291 / 9379 = 3.1027%
Layer #45: 290 / 9380 = 3.0917%
Layer #46: 291 / 9382 = 3.1017%
Layer #47: 291 / 9381 = 3.1020%
Layer #48: 289 / 9380 = 3.0810%
Layer #49: 291 / 9380 = 3.1023%
Layer #50: 290 / 9380 = 3.0917%
Layer #51: 289 / 9379 = 3.0814%
Layer #52: 291 / 9379 = 3.1027%
Layer #53: 291 / 9380 = 3.1023%
Layer #54: 291 / 9380 = 3.1023%
Layer #55: 291 / 9379 = 3.1027%
Layer #56: 291 / 9381 = 3.1020%
Layer #57: 291 / 9381 = 3.1020%
Layer #58: 291 / 9379 = 3.1027%
Layer #59: 291 / 9379 = 3.1027%
Layer #60: 291 / 9379 = 3.1027%
Layer #61: 366 / 9380 = 3.9019%
