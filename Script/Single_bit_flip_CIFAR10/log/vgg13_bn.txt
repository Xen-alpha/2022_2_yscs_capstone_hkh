============================ PYTORCHFI INIT SUMMARY ==============================

Layer types allowing injections:
----------------------------------------------------------------------------------
   - all

Model Info:
----------------------------------------------------------------------------------
   - Shape of input into the model: (3 32 32 )
   - Batch Size: 256
   - CUDA Enabled: True

Layer Info:
----------------------------------------------------------------------------------
Layer #       Layer type  Dimensions         Weight Shape         Output Shape
----------------------------------------------------------------------------------
    0         Identity           4       ['No weights']       [1, 3, 32, 32]
    1           Conv2d           4        [64, 3, 3, 3]      [1, 64, 32, 32]
    2      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    3             ReLU           4       ['No weights']      [1, 64, 32, 32]
    4           Conv2d           4       [64, 64, 3, 3]      [1, 64, 32, 32]
    5      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    6             ReLU           4       ['No weights']      [1, 64, 32, 32]
    7        MaxPool2d           4       ['No weights']      [1, 64, 16, 16]
    8           Conv2d           4      [128, 64, 3, 3]     [1, 128, 16, 16]
    9      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   10             ReLU           4       ['No weights']     [1, 128, 16, 16]
   11           Conv2d           4     [128, 128, 3, 3]     [1, 128, 16, 16]
   12      BatchNorm2d           4                [128]     [1, 128, 16, 16]
   13             ReLU           4       ['No weights']     [1, 128, 16, 16]
   14        MaxPool2d           4       ['No weights']       [1, 128, 8, 8]
   15           Conv2d           4     [256, 128, 3, 3]       [1, 256, 8, 8]
   16      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   17             ReLU           4       ['No weights']       [1, 256, 8, 8]
   18           Conv2d           4     [256, 256, 3, 3]       [1, 256, 8, 8]
   19      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   20             ReLU           4       ['No weights']       [1, 256, 8, 8]
   21        MaxPool2d           4       ['No weights']       [1, 256, 4, 4]
   22           Conv2d           4     [512, 256, 3, 3]       [1, 512, 4, 4]
   23      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   24             ReLU           4       ['No weights']       [1, 512, 4, 4]
   25           Conv2d           4     [512, 512, 3, 3]       [1, 512, 4, 4]
   26      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   27             ReLU           4       ['No weights']       [1, 512, 4, 4]
   28        MaxPool2d           4       ['No weights']       [1, 512, 2, 2]
   29           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   30      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   31             ReLU           4       ['No weights']       [1, 512, 2, 2]
   32           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   33      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   34             ReLU           4       ['No weights']       [1, 512, 2, 2]
   35        MaxPool2d           4       ['No weights']       [1, 512, 1, 1]
   36  AdaptiveAvgPool2d           4       ['No weights']       [1, 512, 1, 1]
   37           Linear           2          [4096, 512]            [1, 4096]
   38             ReLU           2       ['No weights']            [1, 4096]
   39          Dropout           2       ['No weights']            [1, 4096]
   40           Linear           2         [4096, 4096]            [1, 4096]
   41             ReLU           2       ['No weights']            [1, 4096]
   42          Dropout           2       ['No weights']            [1, 4096]
   43           Linear           2           [10, 4096]              [1, 10]
==================================================================================


===== Result =====
Quantization bits: 32
Layer #0: 310 / 9405 = 3.2961%
Layer #1: 312 / 9405 = 3.3174%
Layer #2: 311 / 9406 = 3.3064%
Layer #3: 312 / 9406 = 3.3170%
Layer #4: 315 / 9407 = 3.3486%
Layer #5: 312 / 9407 = 3.3167%
Layer #6: 310 / 9407 = 3.2954%
Layer #7: 311 / 9406 = 3.3064%
Layer #8: 310 / 9405 = 3.2961%
Layer #9: 312 / 9406 = 3.3170%
Layer #10: 311 / 9406 = 3.3064%
Layer #11: 311 / 9405 = 3.3068%
Layer #12: 312 / 9407 = 3.3167%
Layer #13: 314 / 9406 = 3.3383%
Layer #14: 311 / 9409 = 3.3053%
Layer #15: 311 / 9406 = 3.3064%
Layer #16: 311 / 9407 = 3.3060%
Layer #17: 312 / 9406 = 3.3170%
Layer #18: 313 / 9405 = 3.3280%
Layer #19: 313 / 9405 = 3.3280%
Layer #20: 312 / 9406 = 3.3170%
Layer #21: 314 / 9406 = 3.3383%
Layer #22: 312 / 9406 = 3.3170%
Layer #23: 314 / 9405 = 3.3386%
Layer #24: 311 / 9406 = 3.3064%
Layer #25: 312 / 9406 = 3.3170%
Layer #26: 313 / 9405 = 3.3280%
Layer #27: 312 / 9406 = 3.3170%
Layer #28: 312 / 9407 = 3.3167%
Layer #29: 311 / 9406 = 3.3064%
Layer #30: 311 / 9405 = 3.3068%
Layer #31: 310 / 9405 = 3.2961%
Layer #32: 311 / 9405 = 3.3068%
Layer #33: 310 / 9405 = 3.2961%
Layer #34: 311 / 9405 = 3.3068%
Layer #35: 310 / 9406 = 3.2958%
Layer #36: 314 / 9405 = 3.3386%
Layer #37: 312 / 9406 = 3.3170%
Layer #38: 312 / 9405 = 3.3174%
Layer #39: 312 / 9405 = 3.3174%
Layer #40: 312 / 9406 = 3.3170%
Layer #41: 311 / 9405 = 3.3068%
Layer #42: 309 / 9407 = 3.2848%
Layer #43: 452 / 9407 = 4.8049%
