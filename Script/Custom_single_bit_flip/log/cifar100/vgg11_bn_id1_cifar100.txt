============================ PYTORCHFI INIT SUMMARY ==============================

Layer types allowing injections:
----------------------------------------------------------------------------------
   - all

Model Info:
----------------------------------------------------------------------------------
   - Shape of input into the model: (3 32 32 )
   - Batch Size: 256
   - CUDA Enabled: True

Layer Info:
----------------------------------------------------------------------------------
Layer #       Layer type  Dimensions         Weight Shape         Output Shape
----------------------------------------------------------------------------------
    0         Identity           4       ['No weights']       [1, 3, 32, 32]
    1           Conv2d           4        [64, 3, 3, 3]      [1, 64, 32, 32]
    2      BatchNorm2d           4                 [64]      [1, 64, 32, 32]
    3             ReLU           4       ['No weights']      [1, 64, 32, 32]
    4        MaxPool2d           4       ['No weights']      [1, 64, 16, 16]
    5           Conv2d           4      [128, 64, 3, 3]     [1, 128, 16, 16]
    6      BatchNorm2d           4                [128]     [1, 128, 16, 16]
    7             ReLU           4       ['No weights']     [1, 128, 16, 16]
    8        MaxPool2d           4       ['No weights']       [1, 128, 8, 8]
    9           Conv2d           4     [256, 128, 3, 3]       [1, 256, 8, 8]
   10      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   11             ReLU           4       ['No weights']       [1, 256, 8, 8]
   12           Conv2d           4     [256, 256, 3, 3]       [1, 256, 8, 8]
   13      BatchNorm2d           4                [256]       [1, 256, 8, 8]
   14             ReLU           4       ['No weights']       [1, 256, 8, 8]
   15        MaxPool2d           4       ['No weights']       [1, 256, 4, 4]
   16           Conv2d           4     [512, 256, 3, 3]       [1, 512, 4, 4]
   17      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   18             ReLU           4       ['No weights']       [1, 512, 4, 4]
   19           Conv2d           4     [512, 512, 3, 3]       [1, 512, 4, 4]
   20      BatchNorm2d           4                [512]       [1, 512, 4, 4]
   21             ReLU           4       ['No weights']       [1, 512, 4, 4]
   22        MaxPool2d           4       ['No weights']       [1, 512, 2, 2]
   23           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   24      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   25             ReLU           4       ['No weights']       [1, 512, 2, 2]
   26           Conv2d           4     [512, 512, 3, 3]       [1, 512, 2, 2]
   27      BatchNorm2d           4                [512]       [1, 512, 2, 2]
   28             ReLU           4       ['No weights']       [1, 512, 2, 2]
   29        MaxPool2d           4       ['No weights']       [1, 512, 1, 1]
   30           Linear           2           [512, 512]             [1, 512]
   31             ReLU           2       ['No weights']             [1, 512]
   32          Dropout           2       ['No weights']             [1, 512]
   33           Linear           2           [512, 512]             [1, 512]
   34             ReLU           2       ['No weights']             [1, 512]
   35          Dropout           2       ['No weights']             [1, 512]
   36           Linear           2           [100, 512]             [1, 100]
==================================================================================


===== Result =====
Seed: 12345678
Layer #0: 967 / 7066 = 13.6853%, Identity
Layer #1: 901 / 7066 = 12.7512%, Conv2d
Layer #2: 922 / 7066 = 13.0484%, BatchNorm2d
Layer #3: 896 / 7066 = 12.6804%, ReLU
Layer #4: 923 / 7066 = 13.0626%, MaxPool2d
Layer #5: 842 / 7066 = 11.9162%, Conv2d
Layer #6: 894 / 7066 = 12.6521%, BatchNorm2d
Layer #7: 884 / 7066 = 12.5106%, ReLU
Layer #8: 934 / 7066 = 13.2182%, MaxPool2d
Layer #9: 861 / 7066 = 12.1851%, Conv2d
Layer #10: 882 / 7066 = 12.4823%, BatchNorm2d
Layer #11: 879 / 7066 = 12.4399%, ReLU
Layer #12: 840 / 7066 = 11.8879%, Conv2d
Layer #13: 839 / 7066 = 11.8738%, BatchNorm2d
Layer #14: 847 / 7066 = 11.9870%, ReLU
Layer #15: 883 / 7066 = 12.4965%, MaxPool2d
Layer #16: 883 / 7066 = 12.4965%, Conv2d
Layer #17: 846 / 7066 = 11.9728%, BatchNorm2d
Layer #18: 841 / 7066 = 11.9021%, ReLU
Layer #19: 847 / 7066 = 11.9870%, Conv2d
Layer #20: 834 / 7066 = 11.8030%, BatchNorm2d
Layer #21: 835 / 7066 = 11.8172%, ReLU
Layer #22: 863 / 7066 = 12.2134%, MaxPool2d
Layer #23: 816 / 7066 = 11.5483%, Conv2d
Layer #24: 822 / 7066 = 11.6332%, BatchNorm2d
Layer #25: 844 / 7066 = 11.9445%, ReLU
Layer #26: 893 / 7066 = 12.6380%, Conv2d
Layer #27: 885 / 7066 = 12.5248%, BatchNorm2d
Layer #28: 865 / 7066 = 12.2417%, ReLU
Layer #29: 942 / 7066 = 13.3314%, MaxPool2d
Layer #30: 916 / 7066 = 12.9635%, Linear
Layer #31: 915 / 7066 = 12.9493%, ReLU
Layer #32: 910 / 7066 = 12.8786%, Dropout
Layer #33: 1020 / 7066 = 14.4353%, Linear
Layer #34: 1008 / 7066 = 14.2655%, ReLU
Layer #35: 1040 / 7066 = 14.7184%, Dropout
Layer #36: 1210 / 7066 = 17.1243%, Linear
